{"title": "Can Lucky Planets Get a Second Chance at Life?", "date": "2023-10-05 12:00:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nWorlds around red giant stars—and others that don’t orbit any star at all—hint at an unexpected diversity of possibilities for planets and life in the universe\nFor decades, astronomers have endeavored to forecast with confidence the fate of planetary systems, including our own, throughout the cosmos. And these experts’ predictions have one central principle: to confidently guess what will eventually befall a planet, you have to know the size of its star.\nTiny stars don’t really burn out but rather fade away as they shine dimly for hundreds of billions or even trillions of years, likely keeping their planetary companions in tow. Massive stars go out with a bang, expiring as a supernova that leaves behind a neutron star or black hole. Such events tend to be cataclysmic for planetary systems. And stars of middling mass, like our own, expand into a red giant, engulfing or scorching their planets and then dissipating to become a slow-cooling stellar ember called a white dwarf.\nThis dismal fate is expected to befall our sun in some five billion years, setting what has been considered the last-gasp expiration date for life on Earth and perhaps throughout the solar system.\nBut insights from fresh studies of dying stars and doomed worlds elsewhere in the Milky Way challenge this consensus. Increasingly, it seems that the eventual fates of planetary systems, ours included, are not wholly written in the stars.\nSpecifically, two new findings—the discovery of a giant planet closely orbiting around a red giant star and the identification and estimation of the number of so-called rogue planets adrift in our galaxy—have highlighted that there are many more nuanced scenarios to consider. Planets can survive the ruin of their star, and the vast majority of planetary systems shed numerous worlds throughout their history.\nWhen our sun eventually enters its red giant phase, its radius will likely extend well beyond Earth’s present-day orbit. Even if our planet and the solar system’s other inner rocky worlds escape engulfment, the sun’s swelling will probably still spell their end because of the scorching temperatures they will experience. For the former scenario, astronomers have been seeing signs of this demise in the atmospheres of white dwarfs: researchers have found such stars littered with the remnants of dead planets they likely swallowed. \nIn fact, astronomers believed the fate of any planet orbiting a star within its red giant radius was likely sealed. That was until the discovery of the planet 8 Ursae Minoris b (8 UMi b), also known as Halla (after the South Korean mountain Hallasan and in honor of the South Korean astronomers who initially identified it in 2015). \n“We used to think that planets just couldn't survive around stars that become red giants—but this system provides a loophole,” explains Malena Rice, an assistant professor of astrophysics at Yale University, who co-authored new research on Halla postulating how it improbably survived.\nHalla was discovered by the wobbling its orbital tugging induced on its red giant home star, 8 Ursae Minoris (8 UMi). Track the period of that wobble over time, and you can discern the length of a planet’s year and its distance from its star. Such scrutiny showed that Halla orbits a mere 75 million kilometers from 8 UMi—that is, just half the distance between Earth and the sun. But standard modeling of 8 UMi’s red giant phase suggested that the star’s puffy, hot stellar atmosphere should have expanded about 30 million km farther out than that at its swollen peak. That is, Halla appeared to be a planet that shouldn’t exist. It should’ve been consumed and obliterated. Instead it had somehow escaped.\n“This planet was very lucky,” Rice says. “In its past, we think that it may have orbited two stars rather than one, and this helped it to survive what could have been a fiery fate.”\nBinary stars can exchange material back and forth, and they can even merge to become a single star, allowing a rich diversity of novel possibilities for any orbiting worlds. Such major redistributions of mass can alter planetary orbits while also profoundly influencing how a star shines, adding or siphoning away gas to change the nature and timing of its subsequent stellar evolution. According to the careful modeling work of Rice and her colleagues, the most likely explanation for Halla’s survival is that 8 UMi was once accompanied by a smaller close-in companion star, with which it eventually merged. Among other effects, the merger would’ve stifled 8 UMi’s red giant expansion, sparing Halla.\nAlthough this mechanism clarifies how some fortunate worlds might survive their star’s antics, it offers scant hope for our own solar system because our sun lacks a stellar companion to tamp down its eventual evolutionary swelling.\n“It will be tough for our rocky planets to make it through that process if the sun swells beyond their orbits,” Rice says. “But perhaps finding more systems like these might teach us about interesting natural ‘loopholes’ that occur in at least some types of planetary systems.” \nBountiful discoveries of newfound worlds—and with them, perhaps, the revelation of more “loopholes”—could come relatively soon via NASA’s Nancy Grace Roman Space Telescope, which is due to launch by May 2027. Much of Roman’s potential comes from its planned exoplanet survey, which will rely on a relatively underused technique known as microlensing. In this method, Roman will stare at many stars simultaneously, looking for instances where, by chance, a planet-bearing star will be perfectly aligned to cross in front of another “background” star much farther away. In such cases, some of the foreground star’s planets can act as gravitational lenses and magnify the background star’s light in a way that allows astronomers to reconstruct a lensing world’s mass and orbit. The technique is especially sensitive to planets orbiting far from their stars—a circumstellar region that remains scarcely probed by other planet-hunting methods.\nAnd in fact, it’s also capable of finding worlds that have left their stars behind entirely—something Roman could leverage to discover hundreds of rogue planets in interstellar space. Already preexisting microlensing surveys have found a handful of these free-floating worlds, and the statistics of this largely hidden population suggest most planetary systems have a surprisingly turbulent history.\nThe latest example comes from the MOA (Microlensing Observations in Astrophysics) survey, a project conducted at the University of Canterbury Mt. John Observatory on New Zealand’s South Island by an international team, including scientists at NASA and Japan’s Osaka University. Running for almost a decade, MOA has gathered enough data to weigh in on the galactic abundance of rogue planets down to and even below Earth mass.\n“This number turns out to be somewhat larger than we would have guessed,” says David Bennett, a senior research scientist at NASA’s Goddard Space Flight Center and co-author of two new papers reporting on these findings that were posted on the preprint server arXiv.org. These papers are set to be published in a future issue of the Astronomical Journal.\nSo far MOA has only detected six microlensing events that are consistent with magnification by a low-mass rogue planet, says MOA collaborator Takahiro Sumi, a professor at Osaka University, who co-authored both preprint studies. “Taking into account the low detection efficiency and our detections, we estimated that there are many such low-mass objects in the galaxy,” he adds. \n“We found that there are about 20 free-floating planets per star in the galaxy, and the number is dominated by low-mass planets with a mass similar to or smaller than that of Earth,” Bennett says. Those numbers, in turn, suggest an astounding two trillion rogue worlds in the Milky Way alone—six times more than the planets that are estimated to be bound to stars.\nIf this estimate is correct, it means most planetary systems are essentially dissolving across cosmic time, jettisoning many of their members via dynamical interactions between planets or their host stars that can slingshot unlucky worlds out into the interstellar abyss. It’s possible that when we look out into the solar system and other multiplanetary systems, the remaining planets we see are rare vestiges of once-bustling neighborhoods.\nBennett explains that most rogue worlds likely get ejected during the early stages of planetary formation, after which planetary systems settle into more stable configurations. The probability of ejections should generally decrease throughout a sunlike star’s life, he says. But when it swells into a red giant and begins shedding its outer layers of gas, the resulting shifts in planetary orbits can spark new rounds of world-ejecting instabilities.\nStars that are much heavier than the sun and end their life as a supernova, Bennett suggests, could also provide a rich source of rogue worlds and help to explain MOA’s outsize estimates.\nScott Gaudi, an astronomer and microlensing expert at the Ohio State University, thinks MOA’s surprising results are the best currently available but cautions that they remain very uncertain, so they “should be taken with a grain of salt.” Roman, he says, should beef up the statistical certitude, thanks to the unprecedented sensitivity of its prospective microlensing survey.\nIf MOA’s estimates are accurate, however, the sheer number of rogue worlds raises an interesting question: Could any of them provide conditions favorable to life? Ravi Kopparapu, a planetary habitability expert at NASA’s Goddard Space Flight Center, says life on a rogue planet would be problematic—but not impossible.\n“Without a star, life on a cold rogue world would likely need to get its energy from internal sources,” Kopparapu says. “That could be in the form of tidal/frictional heat like in some of Jupiter’s moons where there are subsurface oceans, from residual energy when the planet formed or from the radioactive decay of heavy elements in the planet’s core.” Such worlds might resemble the large moons of our outer solar system and harbor potentially clement conditions beneath an icy crust.\nFor surface habitability, Kopparapu says a thick hydrogen atmosphere could possibly insulate a rogue planet and keep its surface temperature warm enough for living things to endure. Such atmospheres are easily blown away by stellar radiation, but because rogue planets do not orbit stars, they might be able to cling to an insulating atmosphere of hydrogen far longer than any sunbathed world could.\nAmid so much uncertainty, life’s prospects in such alien environments can seem either dizzying or dim. Might biospheres someday be found eking out existence around post-red giant stars or on worlds without a star at all? The thought is staggering, to say the least—and the fact that we could soon have real data to better answer such grand questions is all the more so.\nConor Feehly is a New Zealand based writer who covers topics ranging from astronomy to consciousness studies and the philosophy of science. His work has appeared in New Scientist, Discover, Nautilus, Live Science and many other publications.\nNola Taylor Tillman\nCharles Q. Choi and SPACE.com\nMike Wall and SPACE.com\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "Nobel Prizes Are Taking Longer to Award Groundbreaking Research", "date": "2023-10-04 20:00:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nNobel Prize Sale!\nNobel Prize Sale!\nNobel laureates sometimes wait 20 years or more after making their award-worthy discovery to receive the prize\nThe road to a Nobel Prize, the most prestigious scientific award in the world, is growing ever longer, with almost half of laureates now waiting more than 20 years from making a Nobel-worthy discovery to receiving the prize.\nOne analysis shows that the average time between publishing the work and receiving one of the science prizes has nearly doubled in the past 60 years. Across the three science prizes, chemistry now has the longest ‘Nobel lag’ — an average of 30 years over the past decade — and physiology or medicine has the shortest, at 26 years (see ‘Decades-long delay’).\nAlfred Nobel’s will stated that the prizes should be awarded “to those who, during the preceding year, shall have conferred the greatest benefit to mankind.” In reality this has only happened a few times. But in the first half of the twentieth century, it was common for Nobel prize recipients to be in their 30s — and that is unheard of now, says Santo Fortunato, now a computational social scientist at Indiana University in Bloomington, who published a 2014 analysis on Nobel prizewinners since the award’s conception in 1901. His results showed that the time between laureates’ prize-winning research and their Nobel had slowly increased over the years, with a steeper slope after the 1960s than in the early years of the prize.\nThere are a number of possible reasons for this trend, says Yian Yin, a computational social scientist at Cornell University in Ithaca, New York. It could be that the overall number of breakthroughs is increasing each year, so awards cannot keep up with the number of people who deserve to be recognized, he says. It is also the case that the importance of some works, which Yin describes as ‘sleeping beauties,’ are only realized years or decades later.\nAlternatively, the lengthening gap could be a sign that there has been a decrease in ‘disruptive’ science — important studies or discoveries that change the paradigm of their field. This could be causing the Nobel committees to focus more on the past.\nThe number of ‘big-splash’ discoveries are diminishing, but when they do happen, they tend to get recognized quickly, says Fortunato. For example, biochemists Jennifer Doudna at the University of California, Berkeley and Emmanuelle Charpentier at the Max Planck Unit for the Science of Pathogens in Berlin, won the 2020 Nobel Prize in Chemistry just eight years after their development of the CRISPR–Cas9 system as a genome-editing tool. Some researchers speculate that the inventors of mRNA vaccines, which were rolled out to millions of people worldwide during the COVID-19 pandemic, could receive similar recognition.\nFortunato points out that, if the gap continues to grow, prominent scientists could miss out on the award owing to the Nobel Committee’s rule banning posthumous prizes (with the exception of the 2011 Nobel Prize in Physiology or Medicine, a share of which was awarded to physician Ralph Steinman, who had passed away three days before the announcement, unbeknownst to the committee). “It has to stop at some point,” he says, adding that a rethink of the posthumous-awarding ban would allow more people’s work to get the recognition that it deserves.\nThis article is reproduced with permission and was first published on September 29, 2023.\nLilly Tozer is an intern at Nature News.\n\nJosh Fischman\nDaniel Garisto\nLauren J. Young\nDina Fine Maron\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "September Was the Most Anomalously Hot Month Ever", "date": "2023-10-04 19:21:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nSeptember shattered a record for the highest temperature anomaly of any month and could help push 2023 to be the first year to exceed 1.5 degrees Celsius above preindustrial temperatures\nIn a year already overloaded with so many climate-related superlatives, it’s time to add another to the list: September was the most anomalously warm month ever recorded.\nAnd the steady heat building this year could make 2023 not only the hottest year on record but the first to exceed 1.5 degrees Celsius (2.7 degrees Fahrenheit) above preindustrial temperatures, or the stable climate that preceded the massive release of greenhouse gases into the atmosphere from burning fossil fuels. Under the landmark Paris climate accord, nations have pledged to try to keep global warming under that threshold. “It’s very worrying,” says Kate Marvel, a senior climate scientist at Project Drawdown, a nonprofit organization that develops roadmaps for climate solutions.\nAccording to data kept by the Japan Meteorological Agency, this September was about 0.5 degree C (0.9 degree F) hotter than the previous hottest September in 2020. It was also about 0.2 degree C (0.4 degree F) warmer than the previous record high temperature anomaly—a measure of how much warmer or colder a given time period is, compared with the average—which had been set in February 2016 during a blockbuster El Niño.\nThe September anomaly “is so far above anything we’ve seen before,” says Zeke Hausfather, a climate scientist who works at the payment processing firm Stripe and wrote about September’s heat in a recent blog post. On X, formerly known as Twitter, he called the feat “absolutely gobsmackingly bananas.”\nThe milestone reached last month comes on the heels of July setting the record for the hottest month overall. (July is always the hottest month of the year globally because it occurs at the peak of the Northern Hemisphere summer. The Northern Hemisphere has much more landmass to soak up the sun’s rays than the Southern Hemisphere, so it has the bigger influence on the global annual temperature cycle.)\nIn a marker of just how much global temperatures have risen in recent decades, Hausfather observes, “this September will be hotter than most Julys before the last decade or two.”\nTwo main factors are at play in driving temperatures to such extremes: their inexorable increase from burning fossil fuels and an El Niño event that is shaping up to be a strong one. El Niño is a part of a natural climate cycle that features a tongue of unusually warm waters across the eastern Pacific Ocean. Those waters release heat into the atmosphere and can cause a cascade of changes to key atmospheric circulation patterns linked to the weather around the world.\nHeat waves have broken records all over the globe during the past few months, including prolonged events called heat domes that plagued the southern stretch of the U.S. and parts of the Mediterranean. Summerlike temperatures were even felt in South America during the Southern Hemisphere’s winter. Two of the heat waves—one in the U.S. Southwest and one in Europe—were found to be virtually impossible without global warming. And summerlike heat has continued in places into October.\nThe most drastic temperature anomalies typically come in the winter months, when El Niño peaks in strength. In fact, the previous most anomalously warm month was February 2016, during one of the strongest El Niños on record. But this year “we’re seeing these [big anomalies] in the Northern Hemisphere summer,” Hausfather says. That leaves open the possibility of even larger anomalies when this event peaks this winter, particularly if it ends up being another strong event.\nIt is possible there is also some influence from the phasing out of sulfur-containing fuels used by ships because the aerosols spewed into the air from burning those fuels tend to have a slight cooling effect. The eruption of the Hunga Tonga–Hunga Haʻapai volcano in the southern Pacific Ocean last year may also be nudging up temperatures because of the huge amounts of water vapor—also a greenhouse gas—it injected into the atmosphere. But both factors have very small influences, compared with climate change and El Niño.\nGiven that this El Niño is expected to persist and likely to strengthen, there’s a good chance that 2023 or 2024—or both—will become the hottest year on record, besting 2016 (and 2020, which some agencies who monitor climate have tied with 2016). That isn’t surprising, given that there has been a tenth of a degree of warming since 2016, though it is “remarkable just how quickly we’ve seen warmth this year,” Hausfather says. Part of the apparent rapid warming is because 2023 began in the tail end of an unusual string of three back-to-back La Niña events. These tend to have a cooling impact on the global climate, though La Niñas today are hotter than even El Niños of several decades ago.\nBeyond potentially becoming the hottest year on record, 2023 could also be the first year to top 1.5 degrees C above preindustrial temperatures (some individual months have already passed that threshold). But even if that happens, all hope is not lost for meeting the Paris accord goals. That threshold is measured as an average of several decades, and climate scientists have long expected that a single year would pass that mark a decade or so before the world could be considered permanently above that limit. “There is still time to limit global warming to 1.5 degrees,” Marvel says. “It is going to be incredibly difficult. The pathways are narrowing.”\nBut this year should be considered a warning of the future we face if we don’t take rapid, ambitious action. “This is what the world looks like when it’s 1.5 degrees hotter in a year, and it’s terrible,” she says. When the world does permanently pass 1.5 degrees C, the climate anomalies for individual years will reach higher than that mark.\nTo stave off that future, every bit of carbon we can keep, or take, out of the atmosphere is crucial. “Every tenth of a degree matters,” Hausfather says.\nAndrea Thompson, an associate editor at Scientific American, covers sustainability. Follow Andrea Thompson on Twitter Credit: Nick Higgins\nMeghan Bartels\nStephanie Pappas\n\nKatherine Harmon\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "Dengue's Spread in Europe Could Spur Vaccine Development", "date": "2023-10-04 17:00:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nNobel Prize Sale!\nNobel Prize Sale!\nBut dengue in wealthy countries could divert medication away from poorer nations that may need it more\nCLIMATEWIRE | In the early morning of the last day of August, Parisians experienced for the first time a practice normally confined to tropical regions — authorities fumigating the city against the tiger mosquito. The event was a tangible confirmation of what public health stats already showed: Dengue, the deadly mosquito-borne disease, had well and truly arrived in Europe.\nIn 2022, Europe saw more cases of locally acquired dengue than in the whole of the previous decade. The rise marks both a public health threat and a corresponding market opportunity for dengue vaccines and treatments; news that should spur the pharmaceutical industry to boost investment into the neglected disease.\nOn the face of it, this shift would appear to benefit not only countries like France but also nations like Bangladesh and the Philippines that have long battled dengue.\nBut that assumption could be fatally flawed, experts told POLITICO.\nPeople working in the field say the rise of dengue in the West could, in fact, make it harder to get lifesaving drugs to those who need them most, because pharmaceutical companies develop tools that are less effective in countries where the dengue burden is the highest or because wealthy nations end up hoarding these medicines and vaccines.\n“It might look like a good thing — and it is a good thing — that we're getting more products developed, but does it then create a two-tier system where high-income populations get access to it and then we still have the access gap for low- and middle-income countries?” asked Lindsay Keir, director of the science and policy advisory team at think tank Policy Cures Research.\nClimate change and migration mean the mosquitoes that transmit dengue, as well as other diseases such as chikungunya and Zika, are setting up shop in Europe. The most recent annual data from the European Centre for Disease Prevention and Control shows that, in 2022, Europe saw 71 cases of locally acquired dengue: 65 in France and six in Spain.\nWhile dengue usually results in mild or no symptoms, it can also lead to high fever, severe headache and vomiting. Severe dengue can cause bleeding from the gums, abdominal pain and, in some cases, death.\nSo far, the mosquito has mostly been confined to southern Europe, but it's a worry across the Continent. In Belgium, the national public health research institute Sciensano has even launched an app where members of the public can submit photos of any Asian tiger mosquitos they spot.\nThe diseases spread by these mosquitoes have traditionally fallen under the umbrella of neglected tropical diseases, a group of infections that affect mainly low-income countries and struggle to attract research and development investment. But this is changing.\nPolicy Cures Research, which publishes an annual report on R&D investment into neglected diseases, removed dengue vaccines from their assessment in 2013. Dengue was no longer seen as an area where there was market failure, due to the emergence of a market that the private sector could tap into.\nThe organization is still tracking dengue drugs and biologics and their 2022 analysis showed a 33 percent increase in funding for research into non-vaccine products compared to the previous year, with industry investment reaching a record high of $28 million.\nSibilia Quilici, executive director of the vaccine maker lobby group Vaccines Europe, said the most recent pipeline review of members found that roughly 10 percent were targeting neglected diseases. There is more R&D happening in this area, said Quilici.\nAcross the major drugmakers, J&J is working on a dengue antiviral treatment and MSD has a dengue vaccine in their pipeline, while Sanofi has a second yellow fever jab in development. Two dengue vaccines are already approved in the European Union — one from Sanofi and another from Takeda. Moderna recently told POLITICO that it is looking closely at a dengue vaccine candidate and it already has a Zika candidate in the works.\nBut just because there might soon be larger markets for major pharmaceutical companies doesn’t mean the products will be suitable for the populations that have been waiting years for these tools.\nRachael Crockett, senior policy advocacy manager at the non-profit Drugs for Neglected Diseases initiative (DNDi), said increased pharmaceutical investment in a particular disease won't necessarily lead to products developed that are globally relevant. “Industry will — and governments are also more likely to — focus on prevention,” she said.\nThat means tools such as vaccines will be prioritized; but in countries where dengue is endemic, the rainy season completely overburdens their health systems and what they desperately need are treatments, said Crockett.\nShe also said a massive increase in investment without a structure to ensure access to resulting products means “we have absolutely no guarantee that there isn't going to be hoarding, [that] there isn't going to be high prices.” Case in point: The U.S. national stockpile of Ebola vaccines, which exists despite there never having been an Ebola outbreak in the country.\nUnderlying many of these fears are the mistakes of the Covid-19 pandemic, which saw countries with less cash and political heft at the back of the queue when it came to vaccines.\nLisa Goerlitz, head of German charity Deutsche Stiftung Weltbevölkerung’s Brussels office, warned if drug development picks up because of a growing market in high-income countries, then accessibility, affordability and other criteria that make it suitable for low resource settings might not be prioritized.\nVaccines Europe’s Quilici sought to allay these concerns, pointing to the pharmaceutical industry's Berlin Declaration, a proposal to reserve an allocation of real-time production of vaccines in a health crisis. Quilici said this was a “really strong commitment … which comes right from the lessons learnt from Covid-19 and which could definitely overcome the challenges we had during the pandemic, if it is taken seriously.”\nReprinted from E&E News with permission from POLITICO, LLC. Copyright 2023. E&E News provides essential news for energy and environment professionals.\nAshleigh Furlong is a contributor at E&E News.\n\nSeema Yasmin and Madhusree Mukerjee\nMeghan Bartels\nHelen Branswell and STAT\nHarini Barath\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "Entangled Light from Multitasking Atoms Could Spark Quantum Breakthroughs", "date": "2023-10-04 13:00:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nA colorful quirk of quantum optics could lead to significant advances in quantum communication and computing\nDriving late at night, you come upon a red light and stop the car. You lift your hand wearily to block the red glow streaming through your windshield. Suddenly, both the green and yellow lights come on, hitting your eyeballs at the same time. Confused, you take your hand away, and again only the red color appears.\nThis surreal scenario is what would actually happen if the traffic light was a single atom illuminated by a laser beam, as recently shown experimentally by researchers in Berlin. They looked at the light scattered by an atom and saw that photons—the tiniest particles of light—arrived at the detector one at a time. The scientists blocked the brightest color they saw, and suddenly pairs of photons of two slightly different colors started arriving at their detector simultaneously. They reported their findings in Nature Photonics in July.\nThe reason for this counterintuitive effect is that single atoms are skilled little multitaskers. Through different underlying processes, they can scatter a variety of colors at the same time like a dangerous traffic light that shines all three colors at once. Yet because of quantum interference between these processes, an observer only sees one of the metaphorical traffic light’s colors at a time, preserving peace on the road.\nThis experiment also paves the way for novel quantum information applications. When the brightest color is blocked, the photons that pop up simultaneously are entangled with each other, behaving in sync even when they are separated over large distances. This provides a new tool for quantum communication and information processing in which entangled photon pairs can serve as distributed keys in quantum cryptography or store information in a quantum memory device.\nAtoms can be surprisingly picky about their couplings with light. Based on the varying arrangements of their constituent electrons, atoms of different elements each display clear preferences for which colors of light they strongly scatter. Proving as much is as simple as shining a laser at an atom, with the laser tuned to a particular color that closely matches that atom’s scattering preference. As expected, your detector will show the atom scattering photons of that predominant color. But strangely, the scattered photons will stream into the detector one at a time, as if in a single-file line. Up through the early 1980s physicists generally accepted a naive explanation for this strange effect: the photons arrive as if in a queue because the atom can only scatter one photon at a time.\nIn 1984, however, two researchers dug into the math governing this phenomenon and found that the reality is much more complicated—and much more inherently quantum. They theorized that the atom is actually doing many things simultaneously: scattering not only single photons but also, through an entirely different process, photonic pairs, triplets and quadruplets. Nevertheless, only one photon at a time arrives at the detector because of quantum interference among these processes.\nRegular interference occurs between two waves like ripples on a pond, overlapping in a pattern of crests and troughs. A distinctive feature of the quantum world is that interference occurs not only between actual waves but also between probabilities: a photon sent through two slits has some probability of going through the left slit and some probability of going through the right one. The two possible paths interfere with each other, forming a pattern of crests and troughs. Block either slit, and the pattern disappears. “I like to tell my students, ‘Imagine that you want to prevent a burglar from entering your house and going into the living room. Just leave two doors open, and then you will have destructive interference, and the thieves cannot go into the living room,’” jokes physicist Jean Dalibard, who co-authored the 1984 paper.\nIn Dalibard’s model, however, this interference is not a joke at all. It actually happens between the two underlying processes, the single-photon and multiphoton scattering. And it happens not in space but in time such that a probability trough appears for two photons arriving at the same time. So the atom multitasks, yet it does so in a way that looks suspiciously like doing just one thing.\nDalibard’s complex description of the multitasking atom languished in relative obscurity until recently. “I was very happy that the group from Berlin found this paper. I don’t know how they did,” he says. From their end, the researchers in Berlin were fascinated by the counterintuitive theory introduced by Dalibard and his co-author, physicist Serge Reynaud. “When we started to dig into the old literature from the 1980s, we really got intrigued,” says Max Schemmer, a former postdoctoral researcher at Humboldt University of Berlin and a co-author of the recent work.\nSchemmer and his colleagues saw the potential of recently developed technology to experimentally test this theory. First, they cooled a cloud of rubidium atoms to just shy of absolute zero. Then they used optical tweezers—a tightly focused laser beam strong enough to grab extremely tiny objects—to isolate and hold one atom. Next they illuminated that atom with another laser tuned to rubidium’s scattering preference and placed a lens off to the side to collect the scattered light and channel it into an optical fiber.\nTo block the brightest color, the researchers guided the light into a finely tuned filter created by a ring of optical fiber. The length of the ring was chosen and adjusted precisely to create destructive interference for only one color of light. When this filter was included in the light’s path, they saw the brightest color disappear. And as Dalibard and Reynaud had predicted, photons of two slightly different colors suddenly started arriving at the detector in simultaneous pairs.\nBy blocking the brightest color, thus taking the atom’s single-photon-generating process offline, Schemmer and his colleagues were able to see the other process in action without the destructive interference created by the dominant single atom—much like a traffic light that shines both green and yellow when red is blocked.\nThe atom’s “second task” of scattering photons in pairs could come in handy for quantum computing and communication. Once the brightest color is blocked, the pairs of photons that arrive simultaneously are entangled with each other—entanglement being the not-so-secret ingredient that gives quantum approaches advantages over classical ones.\nEntangled photon pairs could be used to share quantum information across vast distances or to transmit it between different mediums. Conveniently, the photon pairs produced with this technique come in a very precise color rather than being spread across larger chunks of the rainbow like photon pairs produced by conventional methods. This makes them particularly useful for efficiently storing quantum information in a quantum memory device, Schemmer says, which could in turn lead to more robust quantum communication networks.\nAdditionally, these photon pairs possess a unique kind of entanglement that is not offered by other sources: a syncing in time. “There is one existing technique of producing entangled pairs of photons,” says Magdalena Stobinska, a quantum optics expert, who did not participate in the work. “But this is a different degree of freedom and therefore can be used for different types of applications. So it broadens the palette of efficiently produced entangled pairs of photons. And I think that’s cool.”\nAnd theory predicts that photon pairs are not the end of the story. The atom is also simultaneously scattering entangled photons in threes, fours, and so on. Blocking the red on this “traffic light” makes not only yellow and green shine through but also blue, orange, and much more. Clusters of entangled photons created this way could potentially serve as resources for photon-based quantum computing. “This system is like a treasure trove of quantum correlations,” says Fabrice P. Laussy, a professor of light-matter interactions at the University of Wolverhampton in England, who reviewed the recent study but did not participate in the research. “Everything is in there.”\nDina Genkina is a Brooklyn, N.Y.–based freelance journalist and a science communicator at the Joint Quantum Institute.\nKatherine Wright\nChris Ferrie | Opinion\nAnil Ananthaswamy\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "A Popular Decongestant Doesn't Work. What Does?", "date": "2023-10-04 12:00:00", "text": "The popular decongestant phenylephrine is not effective, an FDA panel found. Here’s what to use instead.\nTanya Lewis: Hi, this is Your Health, Quickly, a Scientific American podcast series!\nJosh Fischman: We bring you the latest vital health news: Discoveries that affect your body and your mind.  \nLewis: And we break down the medical research to help you stay healthy. I’m Tanya Lewis.\nFischman: I’m Josh Fischman.\nLewis: We’re Scientific American’s senior health editors. \nFischman: Today, we’re talking about decongestants. Scientists who advise the FDA recently concluded that phenylephrine, a common decongestant in cold medicines, doesn’t work. We’ll talk about what actually does.\n[Clip: Show theme music]\nLewis: I don’t know about you, Josh, but I have bad allergies and my sinuses are blocked pretty often. I’ve tried all sorts of things to help, from nasal sprays and decongestants to to antihistamines to hot showers. Some of these things help, some of them don’t.\nFischman: What helps you the most of all those things, Tanya?\nLewis: I find that the steroid nasal sprays work pretty well, but I don’t like to use them all the time.\nFischman: How come?\nLewis: Um, I just find that sometimes I develop a tolerance to it so it stops having the same effect. Sometimes hot showers do help temporarily, but usually the congestion comes back.\nFischman: Yeah, and there’s only so long you can stand under a hot shower, right? \nLewis: Right.\nFischman: I’ve tried those saline sprays up my nose. They kind of flush things out and I feel more comfortable. But I have to use them for a bunch of days before I feel any difference.\nLewis: Yeah, those saline ones are pretty good. \nFischman: Overall, I tend to go for decongestant tablets, which are supposed to reduce swelling inside my nose, opening up my airways.\nLewis: You’re not alone in preferring tablets. One of the most popular decongestant ingredients is phenylephrine. It’s found in drugs like Sudafed PE, Benadryl Allergy D Plus Sinus, and Vicks Dayquil Cold and Flu Relief.\nBut earlier this month, in a rare move, an FDA advisory panel declared that oral phenylephrine is completely useless at clearing up congestion.\nFischman: That really surprised me. I’ve been buying cold and flu medicines for years. And I always look to see if a decongestant like  phenylephrine is in the capsule.\nLewis: I’d heard for a while that it wasn’t that effective, but it’s in a lot of cold medicines. In fact, it became popular because the standard over-the-counter decongestant, pseudoephedrine —the active ingredient in regular Sudafed—got locked up behind pharmacy counters.  That’s because it can be used as an ingredient in making methamphetamine.\nFischman:  I remember that. In the mid-2000s, all these cold medicines were suddenly put behind plexiglass windows with padlocks on them. I had to ask a pharmacist if I wanted some, and there was a limit to how much I could buy.\nLewis: Exactly. So, more products started using phenylephrine. \nFischman: Basically they were using it as a substitute?\nLewis: Yep. Phenylephrine was actually approved in the 1970s, so it had been around a while. But even back then, the FDA said it wasn’t very effective as a decongestant.\nJennifer Le: There was a cough and cold panel in 1972, in which the panel specifically noted that the data were not strongly indicative of efficacy. So this goes back quite a number of years.\nLewis: That’s Jennifer Le, a professor at the pharmacy school at the University of California San Diego. She was on the recent FDA advisory panel earlier this month that made the decision that phenylephrine wasn’t effective.\nBack in the 1970s, the FDA was more concerned with safety than effectiveness.\nLe: So first and foremost, at the dose that's currently approved, 10 milligram for nasal congestion, it does not appear to provide any safety concerns, except in a very small population who has high blood pressure.\nLewis: Then, in 2007, an FDA advisory panel reviewed the data.\nLe: And in reviewing the data they thought that efficacy was maybe suggestive at higher doses, and so the recommendation at that time was to obtain more clinical data. And the committee who reviewed it withdrew approval for those less than 12 years of age.\nLewis: Fast-forward to today, when another FDA panel—the one Le was one—reviewed the drug’s effectiveness again. They looked at more recent data on both how the drug is metabolized and how well it works in people.\nLe: And the pharmacologic data side indicated that when you take oral phenylephrine, most of it is metabolized to inactive forms, so very little of the active drug—in fact, one percent, based on FDA data—actually gets into the blood.\nFischman: So, most of the drug isn’t even making it to the nose, in other words.\nLewis: Exactly. In addition to that, three trials of oral phenylephrine showed it was no better than a placebo at relieving congestion.\nSo, the committee voted unanimously that oral phenylephrine is basically useless.\nFischman: The FDA panel only reviewed forms of the drug that come in capsules, tablets and syrups, though. So what about things like nasal sprays?\nLewis: They didn’t review phenylephrine nasal sprays. Those might still be effective since they are going right into your nose. But the oral pills won’t do much.\nFischman: But I’ve been taking these cold medications with phenylephrine for years and they do make me feel better. I think. Is that just a placebo effect?\nLewis: Not necessarily. Those meds usually are a combo of several ingredients such as acetaminophen, which helps reduce pain and fever, and antihistamines, which help in the first few days. So the combo may still make you feel better.\nFischman: Overall, though, if oral phenylephrine doesn’t work, what should people use instead of it?\nLewis: I asked Le the same question. She basically said that for short-term congestion with a cold, you should just wait it out.\nLe: The nasal congestion that occurs with the common cold is self-limiting. And so if it's possible, and if it's tolerable—I have a very high tolerance rate when it comes to symptoms— let it resolve, let the symptom resolve. You know, there's nasal saline products that can maybe help with congestion a little bit. A warm, hot, bath, a humidifier can help with some of that too. \nFischman: But Tanya, you said you tried a lot of those things, and often they don’t work. \nLewis: Yeah, I find that most of them only offer temporary relief.\nFischman: So are you just supposed to walk around with your nose blocked or running for a week, and a headache pounding, maybe a box of tissues tucked under your chin? \nLewis: I know, right? It really doesn’t seem great. There are other decongestants, like pseudoephedrine, which you can get by asking a pharmacist, like we mentioned earlier. And that works pretty well. You can also use nasal sprays like Afrin, but be careful—if you use those longer than three days, they can cause your symptoms to rebound.\nFischman: What about other sprays like Flonase or Nasacort?\nLewis: Those steroid nasal sprays work pretty well. But ask a doctor if you’re congested for longer than a few days, because you might have chronic inflammation due to allergies.\nFischman: And allergies are a different story, right? \nLewis: Right. For that kind of congestion, you should consult an allergy specialist. The standard therapy involves some combination of oral and nasal antihistamines and nasal steroids like Flonase. In some cases, you can get allergy shots or even surgery.\nFischman: Okay, but for colds, clearly it’s time to restock my medicine chest. Those saline sprays do help me, so maybe some more of those. And if I have a rougher case, it looks like I’m going to ask the drugstore to take out their keys, and open up their pseudoephedrine stash. \n[CLIP: Show music]\nFischman: Your Health, Quickly is produced by Tulika Bose, Jeff DelViscio, Kelso Harper, Carin Leong, and by us. It’s edited by Elah Feder and Alexa Lim. Our music is composed by Dominic Smith.\nLewis: Our show is a part of Scientific American’s podcast, Science, Quickly. Subscribe wherever you get your podcasts. If you like the show, give us a rating or review!\nAnd if you have ideas for topics we should cover, send us an email at Yourhealthquickly@sciam.com. That’s your health quickly at S-C-I-A-M dot com.\nI’m Tanya Lewis.\nFischman: I’m Josh Fischman.\nLewis: See you next time.\nTanya Lewis is a senior editor covering health and medicine for Scientific American. Follow her on Twitter @tanyalewis314 Credit: Nick Higgins\nJosh Fischman is a senior editor at Scientific American who covers medicine, biology and science policy. He has written and edited about science and health for Discover, Science, Earth, and U.S. News & World Report. Follow Josh Fischman on Twitter.\nCarin Leong is a multimedia intern producing podcasts and videos at Scientific American. Follow Carin Leong on Twitter\nElah Feder is a journalist, audio producer, and editor. Her work has appeared on Science Friday, Undiscovered, Science Diction, Planet Money, and various CBC shows. Follow Elah Feder on Twitter\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "2023 Nobel Prize in Chemistry Goes to Tiny Quantum Dots with Huge Effects", "date": "2023-10-04 11:21:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nThree scientists won the Nobel Prize in Chemistry for their discovery of quantum dots, an entirely new class of material that is used in large-screen TVs and cancer surgery\nThe 2023 Nobel Prize in Chemistry was awarded today to three scientists for the discovery of quantum dots. These are nanoparticles so small that their size controls their many properties, such as their color. And that in turns makes them invaluable in applications ranging from large color displays to energy production.\nThe winners are Moungi Bawendi of the Massachusetts Institute of Technology, Louis Brus of Columbia University and Alexei Ekimov of the firm Nanocrystals Technology in New York State. The three scientists will share the prize of 11 million Swedish kronor, or nearly $1 million.\nProducing a cornucopia of colors, quantum dots are common materials in big television screens today. Essentially they are tiny crystals, but it’s easier to think of each of them as a compressed ball, just a few nanometers in diameter, that contains electrons. The electrons are key to how the dots work. “If you take an electron and put it into a small space, its wave function gets compressed,” meaning the electron has less freedom to move, said Heiner Linke, a member of the Nobel Committee for  Chemistry, at the announcement. The compression allows the electrons to store more energy.\nThe electrons release that energy as photons—packets of light—and those photons will appear as different colors, depending on how much the electrons are squeezed.\nThat change is a quantum effect, one of the mysterious things that happens in the realm of the incredibly small. So, for example, the smallest dots will emit more shorter-wavelength blue light than longer-wavelength red light. Enlarging the dots slightly will change the color composition.\nThe dots are also used in biomedical imaging—to visualize blood vessels feeding tumors—and in solar cells, where they can amplify the energy generated by the panels. Changing their size can also change other properties, such as their melting point.\nBawendi, when reached by phone by the Royal Swedish Academy of Sciences after the announcement, said he was “very surprised..., sleepy, shocked ... and very honored.” The rest of the world may have been slightly less surprised. Bawendi’s name, along with his two colleagues, was leaked in a document sent out by the academy hours before the official announcement. It was a rare crack in what is ordinarily a highly organized and confidential process. Hans Ellegren, secretary-general of the academy, said the organization did not know what had happened.\nThe news, however it came out, was greeted with applause by other chemists. “These remarkable nanoparticles have huge potential to create smaller, faster, smarter devices, increasing the efficiency of solar panels and the brilliance of your TV screen,” said Gill Reid, president of the Royal Society of Chemistry and an inorganic chemist at the University of Southampton in England, in a recent statement. The Nobel “is really exciting and shows how chemistry can be used to solve a range of challenges,” she said.\nAnd while quantum effects are often considered the province of physics, Judith Giordan, a chemist and president of the American Chemical Society, makes a strong case that dots are chemical products. “We own electrons. They’re on every single atom,” she says. And while the effects of confining electrons in tiny spaces were theorized by physicists, “it was chemists who moved them into novel architectures of atoms, who figured out how to actually produce them in the lab and then in manufacturing settings.”\nThe notion of quantum dots first showed up in theories in the 1930s and then stalled for decades. But in the early 1980s Ekimov put nanoparticles of copper chloride in glass and showed that the particle size changed the color of the glass through quantum effects. Several years later Brus achieved similar color alterations with nanoparticles floating freely in a fluid. \nBawendi, in 1993, developed a way to standardize dot production, which opened the field to many other labs and companies. “He made it easy,” says chemist Rigoberto Advincula, who works on nanoscale technology at the University of Tennessee, Knoxville. Bawendi’s lab created a kind of “soup” of other substances that attached to quantum dot seeds and precisely regulated their growth. This made the seeds very “tunable,” in chemistry lingo, Advincula says. It was a simple way to control their size and thus tune them to produce different levels of energy, he adds.\nIn addition to big screens and solar panels, dots are used to adjust the color of LED lights to make them less harsh. Medical scientists are also exploring their use as sensors and probes for hard-to-find molecules in the body. After the announcement, Bawendi said that “it’s just the beginning.”\nJosh Fischman is a senior editor at Scientific American who covers medicine, biology and science policy. He has written and edited about science and health for Discover, Science, Earth, and U.S. News & World Report. Follow Josh Fischman on Twitter.\nJennifer Ouellette\nDaniel Garisto\nLee Billings\nDaniel Garisto\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "What Colors Do Dogs See?", "date": "2023-10-04 10:45:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nCanine experts weigh in on a TikTok dog vision filter and the rich sensory world of humanity’s best friends\nHave you ever wondered whether your dog could see (and appreciate) the striking pink or nuanced teal color of a new toy? Humans on TikTok are using a dog vision filter to help answer this question. With the filter on, you’ll see the world in shades of blue, yellow and gray—the only colors your pup can perceive.\nBut is this really how our furry friends see the world? Not exactly, experts say—there’s way more to your pet’s vision than color perception.\nScientists once thought dogs saw only in black and white. The idea took off in the public imagination in the 1940s, when optometrist Gordon Walls published his influential book The Vertebrate Eye and Its Adaptive Radiation, in which he claimed that dogs could only weakly see color, if at all. The myth was finally debunked in 1989 when ophthalmologist Jay Neitz, then at the University of California, Santa Barbara, and his colleagues discovered that canines could see blues and yellows but not reds and greens. Some humans, about 8 percent of men and 0.5 percent of women, are similarly red-green color-blind.\nIt turns out that dogs possess two types of color-sensing receptors, called cones, in their retinas. This makes them similar to most mammals—including cats, cattle and pigs—and unlike humans, who have three cones.\n“Our work has had a big influence, and lots of people now understand what color vision in dogs really is,” says Neitz, who is now an ophthalmology professor at the University of Washington.\nBut to really understand how dogs see the world, we need to move beyond color, says Sarah-Elizabeth Byosiere, an animal behaviorist and former director of the Thinking Dog Center at Hunter College. While a green or red ball lying on grass would not stand out easily to your pet, it might challenge them to identify it by its other features—such as its movement, shape and the way it reflects light, Byosiere says. That challenge could either be enriching or frustrating. “It all depends on an individual dog’s behavior,” she says.\nIf you’re really trying to imagine the world through the eyes of your dog, you should picture everything a lot blurrier. Most dogs have 20/75 vision, meaning that they must be 20 feet away from an object to see it as well as a human with clear vision who is standing 75 feet away.\n“Everything looks clear and detailed in those [TikTok] videos, but it wouldn’t look quite as clear to dogs,” Neitz mentions.\nBut unlike humans, who see very poorly in low light, canines have evolved to see well in both daytime and nighttime conditions, explains Paul Miller, a veterinary ophthalmologist at the University of Wisconsin–Madison. Though dogs have fewer color-sensing cones than humans, they have more rods, the cells that help with night vision. They even have a unique structure in their eyes called the tapetum lucidum, a mirrorlike membrane that allows them to see in six times less light than humans can. The tapetum, which some other animals, such as cats and cattle, also possess, sits behind the retina and reflects light back onto it, giving the receptors a second chance to gather more visual detail. It’s also the reason your pet’s eyes glow in photos and in the dark.\nAlso important for dogs’ perception is their sense of smell, which is 10,000 to 100,000 times more powerful than that of an average human. This is as true for chihuahuas and pugs as it is for bloodhounds. While humans have about five million smell receptors, dogs possess up to a billion and can communicate with one another with chemical signals. They can pick up odors as far as 12 miles away.\nAnd canines’ mighty sense of smell is inextricably linked to how they see the world. A study published last year in the Journal of Neuroscience revealed that canines’ brain has a direct connection between their olfactory bulb, which processes smell, and their occipital lobe, which processes vision. This integration of sight and smell had not been observed before in any animal species, the authors stated.\nThe results raise the question of whether dogs’ sense of smell is orienting their sight, Miller says. “It’s pretty wild,” adds Miller, who was not involved in the study. “They [may be able to] smell in 3-D.”\nSo while humans may be attuned to the aesthetics of color, dogs simply aren’t, Neitz says. “I’ve had dogs all my life. And I never really felt like, ‘Oh, my God, my poor dog’s world is limited from a color vision standpoint,’” he says. They live in a very rich olfactory world that humans can’t appreciate, Neitz adds.\nWhen it comes to buying toys for our canine companions, we don’t always have to select the two colors they can see: yellow and blue. Byosiere recommends getting one red and one blue toy to enrich your pet’s play. You may want to throw the red one on the green grass so that your pup uses its nose and then throw a blue one so that it uses its eyes.\n“These animals are not deprived in any way,” Byosiere says. “It’s just that they just see the world in a different way.”\nNiranjana Rajalakshmi is a freelance journalist and a former veterinarian based in Ohio. She covers health, animals, biodiversity, and the environment.\nStephanie Pappas\nStephanie Pappas\nPat Shipman | Opinion\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "Cats Are Perfect. An Evolutionary Biologist Explains Why", "date": "2023-10-04 10:00:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nCats have attained evolutionary perfection\nAnjali Goswami thinks cats are perfect—not in the same way that the average cat person might admire their beauty, athleticism and independence of spirit but from a scientific standpoint. Goswami is an evolutionary biologist at the Natural History Museum in London who studies large-scale patterns of evolution in vertebrate animals through time. She contends that cats—from tabbies to tigers—are quintessential products of evolution. I sat down with her to find out why. Her explanation reveals cats, and the meaning of evolutionary success, in a fascinating new light.\n[An edited transcript of the interview follows.]\nWhen I first came across your argument about cats being perfect, my initial thought as a cat fan was “Well, of course they are. Science confirms the obvious.” But then I realized that this was a really interesting idea. How did it come about?\nI was reading a book by Alex Dehgan called The Snow Leopard Project. In it, he mentioned that in this area in Afghanistan where he was setting up a national park, there are several cat species. I thought that was kind of amazing because ecologically, cats all do the same thing. They’re hard-core predators. They’re carnivores. And there are lots of other places around the world where there are multiple species of cats that coexist—not only today but also in deep time. The thing is, although there are lots of species, they all kind of look the same. They’re just big or small. I started thinking about how cats can be so similar.\nTell me more about how they’re similar. I’m thinking of all the breeds of domestic cat, and even within just that species, there seems to be a lot of variation.\nThey have different coat colors, sure. But they all have the same baby heads. They’re round, and they don’t elongate as the animal matures, which is the standard developmental pattern for mammals. Dogs have short, round faces as puppies but long, snouty faces as adults. An adult cat looks pretty much like a baby cat but bigger. With dogs, breeders play off of that developmental variation to create breeds with different face shapes. But because cats don’t have that developmental variation, there isn’t much to play around with other than coat color.\nThis all goes back to the fact that cats are extremely specialized. Every member of the order of mammals known as the carnivorans, including cats and dogs, has an upper fourth premolar and a lower first molar that form what we call the slicing pair, which slices meat. A lot of carnivorans retain molars behind the slicing pair that can grind up stuff such as vegetation. But cats have lost pretty much everything behind their slicing teeth. They might have a little nub, a peg tooth, but it can’t process stuff. This is why foxes are perfectly happy going through garbage, whereas leopards will kill livestock instead.\nIt doesn’t matter if they’re a tiny Bengal cat or a gigantic lion or tiger. They’re still gonna basically look the same. If you handed me a lion or tiger skull, I could not—as a person who’s a pretty solid expert in carnivorans in general—tell you which one it was. Most people would be hard-pressed to tell you. They look nearly identical. That’s how similar cats are. There’s a teeny amount of allometry [disproportionate change in one body part relative to the whole as a consequence of size] if they get really big: a small elongation of the face and an increase in muscle mass. But the variation is nothing compared with what you see in other groups, such as dogs. Ultimately big cats are really similar to small cats, far more so than you would predict.\nWhat does this have to do with being perfect?\nCats have nailed this one thing so well that they all do it and just come up with slightly different sizes. That’s why they’re perfect, evolutionarily. They don’t need variation. They might get bigger or smaller, but they don’t change anything else because they’re doing it just right otherwise. They’re not jacks-of-all-trades; they’re masters of one.\nBears are the anticats. There are only a few species of bear, and they do different things. You’ve got your superspecialized, weird herbivore, the panda [which basically only eats bamboo]. And then you’ve got spectacled bears [which favor fruits and bromeliads]. You’ve got polar bears, which are hypercarnivorous marine mammals, and the omnivorous black bears and grizzlies. And then there are sloth bears, which mostly eat social insects. So almost every single species of bear does something totally different. And they’re just okay at all of it [laughs]. I really do like bears a lot because of that opposite side of things. They’re interesting because they’re so ecologically diverse.\nPeople usually talk about a group’s diversity as a mark of success. But you’re saying it’s the sameness of cat species, their lack of variation, that indicates that they’re evolutionarily successful, or “perfect.”\nCats challenge standard biases in evolutionary biology. People have said to me, “What about bats? What about rodents? These groups have so many species doing all kinds of things.” And I’m like, “Yeah, because they suck.” They haven’t figured out how to do anything well, so they keep trying different things.\nDo any other vertebrate groups measure up to cats in this way?\nMonitor lizards are as awesome as cats. They are the cats of the reptile world. They vary hugely in body size—they have maybe an even bigger body size range than cats do—and they are all utterly identical. They’re also hard-core carnivores.\nYou and your colleagues have been studying skull evolution in a bunch of animals, and you recently published a paper on what you found in mammals. Did you discover anything interesting about cats in the course of that research?\nWe’ve been trying to measure skull shape in a similar way across all tetrapods (vertebrates with four limbs). We’re looking at salamanders and frogs, birds and crocodiles, dinosaurs and mammals and then trying to understand the variation that we see, the speed at which things evolve and the factors that are associated with how fast things evolve. Within mammals, specifically, being social or solitary affects how fast you evolve. Social mammals evolve faster. Cats are notoriously solitary, except for lions. And cats don’t evolve quickly. Compared with other groups, cats are slowly evolving animals.\nThere are lots of things that have tried to be cats—other groups of mammals that have evolutionarily converged on cats. Marsupials have tried to be cats. An extinct group of carnivorans called creodonts have tried to be cats. Weasels have tried to be cats. There’s all kinds of stuff that has tried to be a bit catlike in different ways—mongooses, things like that. But they kind of dip in and dip out of being cats, and they can’t really outcompete cats in their space. They haven’t lasted. All of those things that have tried to be cats, they do other things, too, and those things are fine. But there aren’t a lot of things that are around today that do a very good job of being a cat.\nSo cats are not only perfect but also inimitable.\nYou can’t just casually try to be a cat. You have to commit. Cats have committed to being cats. Everything else is just sort of dabbling, and it doesn’t work.\nKate Wong is a senior editor for evolution and ecology at Scientific American. Follow her on Twitter @katewong Credit: Nick Higgins\nCarlos A. Driscoll, Juliet Clutton-Brock, Andrew C. Kitchener and Stephen J. O'Brien\n\n\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "How AI Could Help China and Russia Meddle in U.S. Elections", "date": "2023-10-03 18:00:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nNobel Prize Sale!\nNobel Prize Sale!\nChatGPT and similar AI programs give propagandists and intelligence agents a powerful new tool for interfering in politics. The clock is ticking on learning to spot this disinformation before the 2024 election\nThe following essay is reprinted with permission from The Conversation, an online publication covering the latest research.\nElections around the world are facing an evolving threat from foreign actors, one that involves artificial intelligence.\nCountries trying to influence each other’s elections entered a new era in 2016, when the Russians launched a series of social media disinformation campaigns targeting the U.S. presidential election. Over the next seven years, a number of countries – most prominently China and Iran – used social media to influence foreign elections, both in the U.S. and elsewhere in the world. There’s no reason to expect 2023 and 2024 to be any different.\nBut there is a new element: generative AI and large language models. These have the ability to quickly and easily produce endless reams of text on any topic in any tone from any perspective. As a security expert, I believe it’s a tool uniquely suited to internet-era propaganda.\nThis is all very new. ChatGPT was introduced in November 2022. The more powerful GPT-4 was released in March 2023. Other language and image production AIs are around the same age. It’s not clear how these technologies will change disinformation, how effective they will be or what effects they will have. But we are about to find out.\nElection season will soon be in full swing in much of the democratic world. Seventy-one percent of people living in democracies will vote in a national election between now and the end of next year. Among them: Argentina and Poland in October, Taiwan in January, Indonesia in February, India in April, the European Union and Mexico in June and the U.S. in November. Nine African democracies, including South Africa, will have elections in 2024. Australia and the U.K. don’t have fixed dates, but elections are likely to occur in 2024.\nMany of those elections matter a lot to the countries that have run social media influence operations in the past. China cares a great deal about Taiwan, Indonesia, India and many African countries. Russia cares about the U.K., Poland, Germany and the EU in general. Everyone cares about the United States.\nAnd that’s only considering the largest players. Every U.S. national election from 2016 has brought with it an additional country attempting to influence the outcome. First it was just Russia, then Russia and China, and most recently those two plus Iran. As the financial cost of foreign influence decreases, more countries can get in on the action. Tools like ChatGPT significantly reduce the price of producing and distributing propaganda, bringing that capability within the budget of many more countries.\nA couple of months ago, I attended a conference with representatives from all of the cybersecurity agencies in the U.S. They talked about their expectations regarding election interference in 2024. They expected the usual players – Russia, China and Iran – and a significant new one: “domestic actors.” That is a direct result of this reduced cost.\nOf course, there’s a lot more to running a disinformation campaign than generating content. The hard part is distribution. A propagandist needs a series of fake accounts on which to post, and others to boost it into the mainstream where it can go viral. Companies like Meta have gotten much better at identifying these accounts and taking them down. Just last month, Meta announced that it had removed 7,704 Facebook accounts, 954 Facebook pages, 15 Facebook groups and 15 Instagram accounts associated with a Chinese influence campaign, and identified hundreds more accounts on TikTok, X (formerly Twitter), LiveJournal and Blogspot. But that was a campaign that began four years ago, producing pre-AI disinformation.\nDisinformation is an arms race. Both the attackers and defenders have improved, but also the world of social media is different. Four years ago, Twitter was a direct line to the media, and propaganda on that platform was a way to tilt the political narrative. A Columbia Journalism Review study found that most major news outlets used Russian tweets as sources for partisan opinion. That Twitter, with virtually every news editor reading it and everyone who was anyone posting there, is no more.\nMany propaganda outlets moved from Facebook to messaging platforms such as Telegram and WhatsApp, which makes them harder to identify and remove. TikTok is a newer platform that is controlled by China and more suitable for short, provocative videos – ones that AI makes much easier to produce. And the current crop of generative AIs are being connected to tools that will make content distribution easier as well.\nGenerative AI tools also allow for new techniques of production and distribution, such as low-level propaganda at scale. Imagine a new AI-powered personal account on social media. For the most part, it behaves normally. It posts about its fake everyday life, joins interest groups and comments on others’ posts, and generally behaves like a normal user. And once in a while, not very often, it says – or amplifies – something political. These persona bots, as computer scientist Latanya Sweeney calls them, have negligible influence on their own. But replicated by the thousands or millions, they would have a lot more.\nThat’s just one scenario. The military officers in Russia, China and elsewhere in charge of election interference are likely to have their best people thinking of others. And their tactics are likely to be much more sophisticated than they were in 2016.\nCountries like Russia and China have a history of testing both cyberattacks and information operations on smaller countries before rolling them out at scale. When that happens, it’s important to be able to fingerprint these tactics. Countering new disinformation campaigns requires being able to recognize them, and recognizing them requires looking for and cataloging them now.\nIn the computer security world, researchers recognize that sharing methods of attack and their effectiveness is the only way to build strong defensive systems. The same kind of thinking also applies to these information campaigns: The more that researchers study what techniques are being employed in distant countries, the better they can defend their own countries.\nDisinformation campaigns in the AI era are likely to be much more sophisticated than they were in 2016. I believe the U.S. needs to have efforts in place to fingerprint and identify AI-produced propaganda in Taiwan, where a presidential candidate claims a deepfake audio recording has defamed him, and other places. Otherwise, we’re not going to see them when they arrive here. Unfortunately, researchers are instead being targeted and harassed.\nMaybe this will all turn out OK. There have been some important democratic elections in the generative AI era with no significant disinformation issues: primaries in Argentina, first-round elections in Ecuador and national elections in Thailand, Turkey, Spain and Greece. But the sooner we know what to expect, the better we can deal with what comes.\nThis article was originally published on The Conversation. Read the original article.\nBruce Schneier is an adjunct lecturer in public policy at Harvard University's Harvard Kennedy School. Follow Bruce Schneier on Twitter\n\nGary Marcus\nLauren Leffer\nSiwei Lyu | Opinion\nCharles Seife | Opinion\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "FEMA Disaster Money Flowing Again after Budget Standoff", "date": "2023-10-03 16:30:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nNobel Prize Sale!\nNobel Prize Sale!\nThe Federal Emergency Management Agency will resume funding long-term rebuilding projects after withholding funds since August\nCLIMATEWIRE | The Federal Emergency Management Agency lifted temporary spending restrictions Monday that had held up $3 billion for states to rebuild from disasters that occurred in recent years.\nFEMA said it would resume giving states money for rebuilding projects because it received $16 billion in the temporary spending bill that Congress and President Joe Biden signed Saturday.\nThe agency, facing a dwindling budget, had stopped funding long-term projects Aug. 29 to save money for emergency costs such as temporary shelters and road clearing immediately after a disaster.\nThe spending restrictions forced FEMA to withhold funding for 2,400 nonemergency projects that the agency had approved such as rebuilding damaged roads and structures. It was the first time FEMA had imposed “immediate needs funding” restrictions since 2017.\nFEMA said it expects to fund all delayed projects “within the next several weeks.”\nThe $16 billion for FEMA’s Disaster Relief Fund was included in a temporary spending package Congress approved one day before the end of fiscal 2023 when most federal employees were to stop working due to a lack of funding.\nBiden had asked Congress in August to give FEMA $16 billion through a special allocation to enable the agency to continue operating in full.\nFEMA’s disaster fund had dwindled to about $2 billion, which can be spent quickly if a wildfire, storm or flood causes significant damage.\nThe agency’s decision to withhold funds did not automatically block newly approved rebuilding projects. But withholding funds forced states and localities to pay for projects themselves and wait for FEMA reimbursement or to delay the start of the projects.\nFEMA typically pays 75 percent of projects that rebuild public facilities after a major disaster.\nReprinted from E&E News with permission from POLITICO, LLC. Copyright 2023. E&E News provides essential news for energy and environment professionals.\nThomas Frank covers the federal response to climate change for E&E News.\n\nAndrea Thompson\nAvery Ellfeldt and E&E News\nThomas Frank and E&E News\nThe Editors\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "Bringing Mars Rocks to Earth Could Cost an Astronomical $11 Billion", "date": "2023-10-03 16:00:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nNobel Prize Sale!\nNobel Prize Sale!\nNASA’s Perseverance rover has collected valuable samples, but a new report says the plan to fetch them is unworkable\nHumanity’s biggest and most ambitious plan to search for extraterrestrial life is about to go back to the drawing board.\nNASA and the European Space Agency (ESA) have been working on a strategy to fly a set of Mars rocks, carefully collected by the Perseverance rover, back to Earth for study. But a new independent assessment of the plan says it can’t be done on current budgets and schedules. The entire project will probably cost between US$8 billion and $11 billion — far more than the roughly $4 billion estimated in a previous independent review report, issued three years ago. And there’s a “near zero probability” of the missions launching in 2027 and 2028, as the space agencies had hoped. Even pushing the launch dates out to 2030 would still cost between US$8 billion and $9.6 billion, the report estimates — comparable with the cost of building the James Webb Space Telescope, the single most expensive astronomy project in history.\nThe report, released on 21 September, stresses that Mars sample return is strategically important to the space agencies, in that it would demonstrate US and European ‘soft power’ at a time when China has also announced plans to bring back rocks from Mars. The mission is also scientifically important: it is the culmination of a decades-long quest to search for life beyond Earth. But the current plan is unworkable, according to the report, which was commissioned by NASA and led by a former NASA manager, Orlando Figueroa.\nNASA says it will put current plans on hold and come up with an alternative strategy by early next year. “It’s going to take a little time for us to assess the path forward,” says Lori Glaze, head of NASA’s planetary-science division in Washington DC. The recommendations from the report are “big”, she says. “They’re not things that can be answered overnight.”\nIn a statement, ESA said it is evaluating how it can adjust its plan while still achieving the overall mission objectives. “We are conducting preliminary studies to assess all options given the various scenarios and will inform member states and coordinate with NASA on the outcome as soon as possible.”\nAs currently envisioned, a Mars sample-return mission would involve NASA building a lander that would fly to the Red Planet to grab up to 30 rock samples, as well as a rocket that would blast off from the Martian surface to carry them into orbit around Mars. ESA would build the spacecraft to retrieve the precious cargo from orbit and fly it back to Earth.\nScientists can analyse the rocks in much greater detail in laboratories on Earth than with the compact instruments available on robotic rovers. The analysis would include hunting for ‘biosignatures’, molecules or other signals of past life in the samples. “These measurements are difficult to do remotely,” says Daniel Glavin, a planetary scientist at NASA’s Goddard Space Flight Center in Greenbelt, Maryland. “You really want the samples back and in the lab.”\nNASA’s Perseverance rover has already collected a bevy of samples from Mars’s Jezero Crater and has even placed ten sealed tubes, containing rock cores, on the ground for possible retrieval. The rover continues to travel around Jezero, gathering more samples that make its collection increasingly valuable as time goes on, the report says. The rocks gathered so far formed in an ancient river delta and lake that were probably once similar to life-friendly environments on Earth.\nMars sample return was one of the highest-ranked priorities recommended for NASA in the last two planetary ‘decadal’ surveys — reports, put together with input from the research community, that aim to guide the direction of US planetary science for the following ten years. But the project has struggled to remain affordable as engineers have refined the designs for the various spacecraft that would be part of the mission. The earlier independent review, which NASA commissioned from experts outside the agency specifically to head off problems with unexpected cost increases, recommended spending $3.8 billion to $4.4 billion on the sample-return project.\nBut that was before engineers had a full sense of what would be involved and hence how much it would cost, Glaze says. And NASA’s Jet Propulsion Laboratory in Pasadena, California, which would lead much of the Mars sample-return project, has struggled with an overstretched work force. This led NASA to delay last year’s planned launch of a separate mission, a spacecraft destined for the asteroid Psyche.\nThere are also questions about how to balance the cost of Mars sample return against other missions in the $3.2-billion budget for NASA’s planetary-science division. The most recent decadal survey, released in 2022, recommended limiting the cost of Mars sample return to no more than 35% of the division’s overall budget. That’s a big challenge as the agency also tries to keep funding going for other priority projects, such as the Dragonfly mission to Saturn’s moon Titan, slated for later this decade, and a mission to Uranus next decade.\nThat means all eyes remain on how to pay for Mars sample return. “The community knew that prioritization of a multi-mission effort and the single most ambitious effort in the history of planetary sciences would have challenges,” says Bethany Ehlmann, a planetary scientist at the California Institute of Technology in Pasadena who helped to lead the most recent decadal survey. “That’s why the [survey] highlights the importance of NASA working with Congress to augment the budget and figure out the appropriate funding profile to get Mars sample return done.”\nThis article is reproduced with permission and was first published on September 27, 2023.\nAlexandra Witze works for Nature magazine.\n\nShannon Hall\nRobin George Andrews\nLeonard David\nJonathan O'Callaghan\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "Beyond Pluto, New Horizons Gets a Reprieve from NASA", "date": "2023-10-03 15:00:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nNASA has reversed course on plans to curtail the New Horizons spacecraft’s planetary science studies following a rebellion among the mission’s leaders\nIt’s lonely out there in the desolation that reigns where NASA’s New Horizons spacecraft now cruises on its one-way trip out of our solar system, with little to pass the time besides sniffing whiffs of plasma and stargazing. After nearly two decades of deep-space operations, the probe is currently more than eight billion kilometers from Earth. And much like our planet itself, the mission’s heyday—a historic encounter with Pluto in 2015 and a 2019 flyby of Arrokoth, the most distant object yet visited by a spacecraft—is receding ever further in the rearview.\nBack on Earth, a battle has raged over the spacecraft’s future. Pluto and Arrokoth alike reside in what’s known as the Kuiper Belt, a remote and mysterious orbital region of icy objects in the outer reaches of our solar system. New Horizons—humanity’s first and so far only robotic emissary to explore the Kuiper Belt—still traverses its depths, dutifully gathering data and somewhat desperately searching for another object to intercept. Yet last year NASA suggested it would end these investigations in an effort to save money, sparking an outcry from astronomers, given that no other spacecraft will explore the Kuiper Belt for decades.\nThat decision, it seems, has been partly reversed. In a statement from NASA posted on September 29, Nicola Fox, associate administrator of the agency’s Science Mission Directorate in Washington, D.C., announced some of New Horizons’ Kuiper Belt science would continue. “The agency decided that it was best to extend operations for New Horizons until the spacecraft exits the Kuiper Belt, which is expected in 2028 through 2029,” Fox said. NASA’s statement noted that the agency would “assess the budget impact of continuing the New Horizons mission so far beyond its original plan of exploration” and that other missions may be affected by the decision. “Future projects may be impacted,” the statement added.\nAlan Stern, a planetary astronomer at the Southwest Research Institute, who leads the New Horizons mission, welcomed the decision. “It is good news for Kuiper Belt exploration and very much welcomed by our team and also by the planetary science community,” he says. Pontus Brandt of the Johns Hopkins University Applied Physics Laboratory (APL) was similarly jubilant. “The community and I are thrilled that this logjam is finally broken,” he says. “This was the right decision for Kuiper Belt science.” Stern notes that some of the finer details are yet to be ironed out, however. It’s not clear, for example, to what extent New Horizons’ studies of the Kuiper Belt will continue, with NASA’s recent statement noting that the agency’s decision “allows for the possibility of using the spacecraft for a future close flyby” of a Kuiper Belt Object (KBO).\nNASA launched the nearly $1-billion New Horizons mission in 2006 on its pioneering voyage to Pluto and the Kuiper Belt. The probe’s arrival at the dwarf planet nine years later was a stunning moment in space exploration, with New Horizons returning breathtaking images of a surprisingly complex world of craggy mountains of ice and seas of frozen nitrogen, as well as snapshots of Pluto’s equally enthralling red-tinted moon Charon. The additional visit to Arrokoth was a lucky bonus, achieved by dint of the KBO’s timely discovery when it was still within reach of the approaching spacecraft’s dwindling propellant reserves. The two flybys produced “spectacular results,” says Jane Luu of the University of Oslo, who co-discovered the Kuiper Belt in 1992.\nAlthough New Horizons’ day-to-day operational needs are modest, they add up to a cost of nearly $10 million per year. Last year NASA approved a mission extension—but only through September 2024 rather than 2025, as requested by Stern and his team. At that point, NASA had planned to end the spacecraft’s planetary science studies in favor of a focus on heliophysics by repurposing New Horizons to exclusively examine how our home star shapes conditions in the outer solar system and toward the hazy boundary with interstellar space. That transition would swap the mission from NASA’s Planetary Science Division to its Heliophysics Division. And given that Stern and his team did not heed the space agency’s request to submit a proposal by November 2022 to dedicate New Horizons solely to heliophysics, the transition would remove them from the mission, too. “We refused to write a proposal that terminated the Kuiper Belt science,” Stern says. “It’s outrageous that you would terminate the only mission purpose-built and sent to the Kuiper Belt while it’s still collecting unique data.”\nSuch a heliocentric shift would have greatly limited the mission’s scientific output, says Jim Green, NASA’s former chief scientist and former head of its planetary science efforts. “It basically pares down the science team to next to nothing and really operates the spacecraft with a minimal cadre,” he says. “From my perspective, if I was the division chief, I would not have made that decision.” He says the reversal was “a good decision” and will “allow the right science for the mission during the right times.”\nThe decision to halt New Horizons’ Kuiper Belt studies originally emerged in 2022 from NASA’s annual review of most of its planetary science missions, a process in which the space agency assesses their current status and future potential. Although this review acknowledged many benefits of New Horizons continuing its current mission, the report also flagged a key weakness. In the absence of a suitable rendezvous target, the spacecraft can only study KBOs from afar—and in far fewer numbers than what various ground-based telescopes can achieve, perhaps less than a dozen. “The proposed studies of [KBOs] are unlikely to markedly improve knowledge,” the review stated, noting the spacecraft’s priorities “should focus on heliophysics and astrophysics.”\nFaith Vilas of the Planetary Science Institute, who led the team that assessed New Horizons for the review, says she and her colleagues did not intend their work to justify ending the mission’s planetary science studies. The team was “being credited, or blamed, for the mission potentially losing the planetary science side of things,” she says. “We didn’t say that. We simply said that all the science together is greater in magnitude than the one portion of science.”\nStern says the mission still has much to offer as it moves through the Kuiper Belt, including feats that cannot be replicated on Earth, such as observing the changing brightness of KBOs as they rotate. “When you do that repeatedly from different angles, you can determine the shape,” he says. “But you can never do that from Earth because you never see the KBOs from significantly different angles.” The spacecraft can also search for binaries—co-orbiting KBOs—in a way Earth-based observers cannot and can collect dust scattered away from distant Kuiper Belt objects. The prospect of visiting a third object remains ever present, too, if a viable target can be found.\nThe spacecraft is projected to exit the known boundaries of the Kuiper Belt in 2028, at which point Stern agrees the Kuiper Belt science could end. “Then I don’t see a reason to continue a planetary science mission,” he says. By some estimates, the spacecraft could continue operating until 2050, when it will be far beyond the generally accepted boundary of interstellar space. At present, no other spacecraft bound for the Kuiper Belt is in development. The next possibility might be Interstellar Probe, a proposal from APL to send a spacecraft to interstellar space. Optimistically assuming Interstellar Probe becomes a reality and launches in 2036, “that would get you out to the same region of space as New Horizons probably within a decade or so,” says Ralph McNutt, who helms the proposal team at APL, “so potentially up to the mid-2040s.”\nIn June Green and other members of the space science community signed a letter to NASA urging the space agency to reconsider its decision and noted “alarm” at the proposed abandonment of Kuiper Belt science. “We ... ask NASA, the Administration, and Congress to reverse course,” they wrote. In September the U.S.-based National Space Society made a similar appeal in its own letter. “Continue New Horizons so we don’t miss out on new discoveries from this rare, perfectly positioned, and fully functional mission,” the letter stated.\nNot all astronomers agree that New Horizons’ remaining Kuiper Belt investigations will be worthwhile, however. Luu says transitioning the mission to a focus on heliophysics and astrophysics would be “a reasonable decision” because ground-based telescopes can surpass the spacecraft’s Kuiper Belt capabilities in many respects, especially by studying many more KBOs at a much faster cadence. “If you just want to use the spacecraft for monitoring KBOs, I would argue it might be better done from the ground,” she says. And the prospects of a third flyby are becoming increasingly remote because no obvious targets have been discovered. “If they find a new candidate, great, but the low-hanging fruits have been picked,” she says.\nMike Brown of the California Institute of Technology, who discovered the object Eris in 2003, which led to Pluto’s demotion from a planet to a dwarf planet, has similar concerns. “These decisions are always tough,” he says. “There is a spacecraft there! It can do unique things! But ultimately it is a zero-sum cost-benefit analysis. Unless there is a new target for a close flyby, it’s hard for me to see why spending a ton of money is justified. If the science can be done on a shoestring, then perhaps that’s fine. But of course, a shoestring in space is probably many full scientific programs on Earth.”\nFor now, New Horizons will continue its studies of the Kuiper Belt—and will remain the only spacecraft likely to do so for many years to come. What knock-on effects its ongoing operations will have on “future projects” alluded to by NASA remains to be seen. Far beyond Pluto, one of our most distant emissaries still speeds on into the unknown.\nJonathan O'Callaghan is an award-winning freelance journalist covering astronomy, astrophysics, commercial spaceflight and space exploration. Follow him on Twitter @Astro_Jonny Credit: Nick Higgins\nJonathan O'Callaghan\nLee Billings\nS. Alan Stern\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "How Can We Trust AI If We Don't Know How It Works", "date": "2023-10-03 14:00:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nNobel Prize Sale!\nNobel Prize Sale!\nTrust is built on social norms and basic predictability. AI is typically not designed with either\nThe following essay is reprinted with permission from The Conversation, an online publication covering the latest research.\nThere are alien minds among us. Not the little green men of science fiction, but the alien minds that power the facial recognition in your smartphone, determine your creditworthiness and write poetry and computer code. These alien minds are artificial intelligence systems, the ghost in the machine that you encounter daily.\nBut AI systems have a significant limitation: Many of their inner workings are impenetrable, making them fundamentally unexplainable and unpredictable. Furthermore, constructing AI systems that behave in ways that people expect is a significant challenge.\nIf you fundamentally don’t understand something as unpredictable as AI, how can you trust it?\nTrust is grounded in predictability. It depends on your ability to anticipate the behavior of others. If you trust someone and they don’t do what you expect, then your perception of their trustworthiness diminishes.\nMany AI systems are built on deep learning neural networks, which in some ways emulate the human brain. These networks contain interconnected “neurons” with variables or “parameters” that affect the strength of connections between the neurons. As a naïve network is presented with training data, it “learns” how to classify the data by adjusting these parameters. In this way, the AI system learns to classify data it hasn’t seen before. It doesn’t memorize what each data point is, but instead predicts what a data point might be.\nMany of the most powerful AI systems contain trillions of parameters. Because of this, the reasons AI systems make the decisions that they do are often opaque. This is the AI explainability problem – the impenetrable black box of AI decision-making.\nConsider a variation of the “Trolley Problem.” Imagine that you are a passenger in a self-driving vehicle, controlled by an AI. A small child runs into the road, and the AI must now decide: run over the child or swerve and crash, potentially injuring its passengers. This choice would be difficult for a human to make, but a human has the benefit of being able to explain their decision. Their rationalization – shaped by ethical norms, the perceptions of others and expected behavior – supports trust.\nIn contrast, an AI can’t rationalize its decision-making. You can’t look under the hood of the self-driving vehicle at its trillions of parameters to explain why it made the decision that it did. AI fails the predictive requirement for trust.\nTrust relies not only on predictability, but also on normative or ethical motivations. You typically expect people to act not only as you assume they will, but also as they should. Human values are influenced by common experience, and moral reasoning is a dynamic process, shaped by ethical standards and others’ perceptions.\nUnlike humans, AI doesn’t adjust its behavior based on how it is perceived by others or by adhering to ethical norms. AI’s internal representation of the world is largely static, set by its training data. Its decision-making process is grounded in an unchanging model of the world, unfazed by the dynamic, nuanced social interactions constantly influencing human behavior. Researchers are working on programming AI to include ethics, but that’s proving challenging.\nThe self-driving car scenario illustrates this issue. How can you ensure that the car’s AI makes decisions that align with human expectations? For example, the car could decide that hitting the child is the optimal course of action, something most human drivers would instinctively avoid. This issue is the AI alignment problem, and it’s another source of uncertainty that erects barriers to trust.\nOne way to reduce uncertainty and boost trust is to ensure people are in on the decisions AI systems make. This is the approach taken by the U.S. Department of Defense, which requires that for all AI decision-making, a human must be either in the loop or on the loop. In the loop means the AI system makes a recommendation but a human is required to initiate an action. On the loop means that while an AI system can initiate an action on its own, a human monitor can interrupt or alter it.\nWhile keeping humans involved is a great first step, I am not convinced that this will be sustainable long term. As companies and governments continue to adopt AI, the future will likely include nested AI systems, where rapid decision-making limits the opportunities for people to intervene. It is important to resolve the explainability and alignment issues before the critical point is reached where human intervention becomes impossible. At that point, there will be no option other than to trust AI.\nAvoiding that threshold is especially important because AI is increasingly being integrated into critical systems, which include things such as electric grids, the internet and military systems. In critical systems, trust is paramount, and undesirable behavior could have deadly consequences. As AI integration becomes more complex, it becomes even more important to resolve issues that limit trustworthiness.\nAI is alien – an intelligent system into which people have little insight. Humans are largely predictable to other humans because we share the same human experience, but this doesn’t extend to artificial intelligence, even though humans created it.\nIf trustworthiness has inherently predictable and normative elements, AI fundamentally lacks the qualities that would make it worthy of trust. More research in this area will hopefully shed light on this issue, ensuring that AI systems of the future are worthy of our trust.\nThis article was originally published on The Conversation. Read the original article.\nMark Bailey is a faculty member and chair of cyber intelligence and data science at the National Intelligence University.\n\nKatie Hafner, Claire Trageser and The Lost Women of Science Initiative\nConor Feehly\nLilly Tozer and Nature magazine\nAndrea Thompson\nAshleigh Furlong and E&E News\nDina Genkina\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "Climate Disruptions Are Especially Dangerous for the Opioid Epidemic", "date": "2023-10-03 12:00:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nDrug users must be considered in health and climate preparedness efforts\nAs debates rage over how we can best protect those more directly affected by climate change, one group remains absent from the discussion: people with an opioid addiction.\nAround three million people in the U.S. have an opioid use disorder or are dependent on heroin. More than 100,000 Americans died of drug overdoses from April 2022 to April 2023, most of which were from illicit fentanyl opioids. Cities with high overdose rates, such as New York City, San Francisco, Calif., and Phoenix, Ariz., are also at the highest risk of extreme heat, drought and flooding, and the opioid epidemic is likely to peak just as climate change’s most brutal effects start to take root.\nDuring environmental crises, drug users frequently experience more social and economic hardships than the rest of the population. These hardships often snowball into poor health outcomes, including death. Climate change will likely disrupt the illegal drug supply chain—making dangerous drugs even more variable and deadly. Soaring temperatures will make people more vulnerable to overdose. Despite this, drug users are typically not considered in public health efforts related to climate change preparedness. This urgently needs to change.\nAuthorities must start thinking about the intersection between climate change and drug use. They must design climate literacy programs for drug users, build a housing policy that considers climate change’s impact and make harm reduction programs more widely available.\nAs part of the Helping to End Addiction Long-Term (HEAL) Initiative, I research various aspects of America’s ongoing opioid epidemic. Despite all the useful data the project is producing, climate change’s influence has been of little interest to researchers in the initiative or scholars in the field. It wasn’t until last year that the National Institutes of Health announced the availability of funds to support research that specifically examines the intersection between climate change and substance use. Unsurprisingly, there has been very little published on the topic, significantly limiting the opportunity to study and develop solutions to stem the unique risks that drug users face. This inertia comes as the United Nations has yet again warned of the dire need for implementable, cost-efficient solutions to climate change and after the opioid epidemic’s status as a public health emergency was renewed earlier this year. The staggering economic costs have also been overlooked: climate-fueled disasters cost the U.S. roughly $151 billion per year, while the financial impacts of the opioid epidemic land at roughly $1.5 trillion per year, about seven times the annual cost of addressing heart disease in this country.\nThe link between environmental disaster and drug use became clear in 2017 when Hurricane Maria caused an estimated 5,000 deaths in Puerto Rico. Amid the typical flurry of media images of flooded homes, downed power lines and uprooted trees, drug use and overdose rates dramatically increased, as did needle reuse, a key driver of HIV, which has been soaring in Puerto Rico. While groups such as older and homeless people or those with disabilities often receive special outreach and tailored resources during weather-related disasters, virtually nothing is done to formally forecast and address the needs of illicit drug users during these events. Indeed, the U.S. government’s health authorities currently provide no official guidance or support for preempting this disastrous domino effect.\nClimate change will exacerbate overdose and death risk through the increasingly intricate opioid supply chain. As supplies become harder to transport, drug users, like other consumers, could see steep increases in their product’s costs and decreases in quality. More opioid users will likely switch to cheaper but far more potent and potentially lethal synthetic substances, such as fentanyl. Also, extreme heat can create a deeply sedative effect, making lethargy, muscle breakdown and dehydration worse, all of which significantly increases the risk of fatal overdose.\nTo get ahead of the collision course, it’s essential for public health authorities to first build greater climate change literacy among drug users. This effort must begin by providing them with tailored educational materials that highlight climate change’s core causes and consequences. Most importantly, these materials must identify drug users’ specific vulnerabilities, given their socioeconomic and geographical circumstances.\nNext, we need to build climate-related emergency reserves for home maintenance programs and for rent and mortgage payment deferrals. Housing insecurity is not only intimately connected to drug use onset but also to overdose risk. America’s housing crisis—which intensified during the COVID pandemic—will deepen as property damage from extreme weather accelerates and unemployment rises. More people will be displaced and unhoused. Housing officials have implemented policies to prevent this before. During the more intense waves of the pandemic, officials turned toward eviction moratoriums, which were highly effective before they were rolled back last year.\nFinally, we need deeper support for harm-reduction services and treatment such as naloxone, an emergency opioid-overdose-reversal medication, and highly effective drug treatments such as Suboxone, which are harder to obtain for people in Black and low-income communities. We can anticipate that by the early 2030s there will be an increasing need for these resources and the safety net that propels them. In addition to increasing overdose rates, a lack of investment in harm reduction will spur a rapid increase in HIV/AIDS and sexually transmitted infections.\nThere is no simple solution to address the intersecting crises of climate change and opioid misuse. The best-case scenario is that local governments and industry can collaborate to improve the flow of resources to drug users and those at risk of developing an opioid use disorder. The hyperstigmatization that drug users experience is likely to keep them right in climate change’s crosshairs. This is an altogether different type of pollution that is nonetheless as elusive as the one driving climate change.\nThis is an opinion and analysis article, and the views expressed by the author or authors are not necessarily those of Scientific American.\nJerel Ezell is an assistant professor of community health sciences at the University of California, Berkeley. He is also director of the Berkeley Center for Cultural Humility and a Fulbright Scholar.\nConor Feehly\nLilly Tozer and Nature magazine\nAndrea Thompson\nAshleigh Furlong and E&E News\nDina Genkina\nTanya Lewis, Josh Fischman, Carin Leong and Elah Feder\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "Mars Sample-Return Missions Could Reduce Tensions with China on Earth", "date": "2023-10-03 11:30:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nThe U.S. may not beat China at retrieving Martian rocks first, according to an independent review board’s conclusion. But the U.S. can still lead with an exchange of samples here on Earth\nIn September NASA’s Mars Sample Return (MSR) independent review board (IRB), led by the agency’s former “Mars czar” Orlando Figueroa, released findings and recommendations about the MSR project, a collaboration between NASA and the European Space Agency (ESA) that means to return the first samples from the Red Planet. The IRB members did a terrific job analyzing, in their words, the “near zero probability” of its current plans and budget succeeding.\nMars Sample Return matters to our nation and space program and to science, the board emphasized. The return of carefully selected samples, such as ones weathered by geothermal vents, as well as sedimentary and aggregated rocks, will allow scientists on Earth to extensively examine their geochemistry and microscopic composition. It could possibly reveal signs of life, or at least the ingredients of life, on Mars. The mission therefore directly addresses the principal question of space exploration—the nature of life in the universe. But the report’s key finding makes it clear that the mission needs a stretched-out, more robust architecture that would delay its launch into the 2030s and the return of samples to the mid- or even late 2030s. “The current MSR architecture is highly constrained and is not sufficiently robust or resilient,” the panel said in its report. NASA’s current sample-return mission plan relies on the aging Perseverance rover, launched in 2020, to collect its samples. A second rover from Europe was planned, but its development was cancelled. There is a backup plan in case Perseverance is not working, but it relies on brand-new helicopters doing something novel: picking up and carrying the samples. A delay and stretch-out of the schedule will make the rover older when it is needed and move the mission into less favorable trajectory opportunities in the 2030s. That raises the cost and risks of the mission and lowers its chances for success. The report concludes, “Other [return mission] architectures may be more robust and more resilient to schedule risk.”\nIn other words: back to the drawing board. If NASA and ESA continue with MSR (and the report strongly recommends that they should), then a more robust plan involving the collection of more samples and including additional hardware (possibly another rover) must be devised. This plan would stretch well into the 2030s. The panel also noted China’s development of its own, much simpler Mars sample-return mission for 2028 or 2030, which is likely to bring samples back to Earth several years before the NASA-ESA mission returns. (The Chinese mission is more of a “grab sample” mission, in which a lander takes samples from the immediate vicinity of its landing site, and it is much shorter in duration than the NASA-ESA one.) This need not be a negative. We can make the most of our more robust mission by engaging with a putative rival. This will allow us to serve diplomacy while serving science.\nAccording to the report, the NASA-ESA plan is much better scientifically. It will be able to obtain many more extraordinarily well-selected samples, based on both years of in situ experience from previous missions and a careful and extensive sampling campaign by the Perseverance rover. The sampling will be far more wide-ranging than that of the Chinese plan, which is limited to one small region around the mission’s landing spot. Nevertheless, it would be beneficial for American and European scientists to be able to analyze a bit of those first samples—both to uncover the intrinsic science they contain and to exercise the extensive plans of the NASA sampling procedures. Similarly, Chinese scientists would benefit enormously if they had some access to the NASA-ESA samples. A sample exchange would benefit both the U.S. and China. And therein lies the opportunity.\nExamining each other’s samples poses no conceivable strategic threat to either country—the likelihood of a microscopic secret inside a Martian rock helping either nation in their military or economic competition is around zero. But cooperating on this Martian investigation could build up a benign and positive scientific relationship that would serve both countries. It would add to our exploration of Mars, and there are no downsides to advancing China’s exploration of Mars. It plays into American strength—our vigorous and successful science experience on Mars—and mitigates the more trivial worry about who will conduct a Mars sample-return mission first. And it provides resiliency to further delays or mission problems.\nOne obstacle would be reluctance springing from a 2011 law barring even the barest NASA cooperation with China without FBI approval. Exchanging samples involves no dangerous interactions with sensitive hardware or software. But this draconian law has so inhibited agency scientists that one privately told me that they were reluctant to even have a cup of coffee with Chinese researchers at space events. The policy allows the U.S. to cooperate in space with Vladimir Putin while ruling it out with the world’s other leading economy. That might suggest that we need to rethink it.\nChina and the U.S. are at an impasse right now—one filled with hostile, mistrusting, edgy geopolitics. Space cooperation among rivals has a distinguished history. Even now, the U.S. and Russia cooperate on the International Space Station, and in the middle of the cold war, we exchanged lunar samples from the Apollo and Luna missions. Moreover, neither the U.S. nor China want to let current foreign policy tensions move toward confrontation. Presidents Joe Biden and Xi Jinping have expressed interest in developing cooperative initiatives, and in the past three months several U.S. Cabinet officials have gone to China to seek such initiatives. Mars certainly could provide one consistent with the long history of international cooperation in space that would support peace and geopolitical stability.\nThis is an opinion and analysis article, and the views expressed by the author or authors are not necessarily those of Scientific American.\nLouis Friedman holds a Ph.D. in aeronautics and astronautics from Massachusetts Institute of Technology. He has worked on deep-space missions at NASA's Jet Propulsion Laboratory in Pasadena, Calif., and he co-founded the Planetary Society with Carl Sagan and Bruce Murray, serving as the organization's executive director for 30 years. He was co-leader of the Keck Institute for Space Studies Asteroid Retrieval Mission and Interstellar Medium Exploration Studies at the California Institute of Technology.\nConor Feehly\nLilly Tozer and Nature magazine\nAndrea Thompson\nAshleigh Furlong and E&E News\nDina Genkina\nTanya Lewis, Josh Fischman, Carin Leong and Elah Feder\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "New 6G Networks Are in the Works. Can They Destroy Dead Zones for Good?", "date": "2023-10-03 10:45:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nNext-generation 6G technology could “enable applications that we may not even imagine today”\nHigh-speed Internet access has become crucial in a world where school, business, personal life and emergency communications increasingly take place through a handheld device. But surprisingly large swaths of the U.S. still lack a speedy-enough broadband or cellular connection. One potential solution could be a sixth-generation cellular network, which experts say will add a space-based system to ground-based coverage options. This 6G network could eventually connect the entire nation to high-speed data—but its development is still in the early stages.\nActivities such as attending video meetings and streaming high-definition video can require download speeds of 25 megabits per second. But in 2019 those speeds were out of reach for 4.4 percent of Americans, according to the most recent Broadband Progress Report from the Federal Communications Commission. That lack of access to reliable Internet is roughly four to five times higher in rural communities (17 percent) and on tribal land (21 percent), respectively, contributing to a digital divide that disproportionately impacts already underserved communities.\nThis summer the federal government took steps to boost connectivity by expanding existing broadband infrastructure. In late June the Biden administration announced a $42.45 billion commitment to the Broadband Equity, Access, and Deployment (BEAD) program, a federal initiative to provide all U.S. residents with reliable high-speed Internet access. The project emphasizes broadband connectivity, but some researchers suggest a more powerful cellular connection could eventually sidestep the need for wired Internet.\nThe 6G network is so early in its development that it is still not even clear how fast that network will be. Each new generation of wireless technology is defined by the United Nations’ International Telecommunication Union (ITU) as having a specific range of upload and download speeds. These standards have not yet been set for 6G—the ITU will likely do so late next year—but industry experts are expecting it to be anywhere from 10 to 1,000 times faster than current 5G networks. It will achieve this by using higher-frequency radio waves than its predecessors. This will provide a faster connection with fewer network delays.\nNo matter how fast the new network turns out to be, it could enable futuristic technology, according to Lingjia Liu, a leading 6G researcher and a professor of electrical and computer engineering at Virginia Tech. “Wi-Fi provides good service, but 6G is being designed to provide even better service than your home router, especially in the latency department, to address the growing remote workforce,” Liu says. This would likely result in a wave of new applications that have been unfathomable at current network speeds. For example, your phone could serve as a router, self-driving cars may be able to communicate with one another almost instantaneously, and mobile devices might become completely hands-free. “The speed of 6G will enable applications that we may not even imagine today. The goal for the industry is to have the global coverage and support ready for those applications when they come,” Liu says.\nAlthough 6G’s theoretical speeds sound exciting, the 5G network that preceded it also claimed to offer a blazing-fast connection. But people in many parts of the world still lack access to 5G infrastructure; even devices designed to take advantage of 5G must include the ability to fall back on 4G and 3G connections if and when those slower networks are the only available options. “The coverage of the 5G cellular network is only about 10 percent of the Earth’s surface right now,” says Jeffrey Andrews, director of 6G@UT, a research center at the University of Texas at Austin that works on underlying technologies to support 6G cellular networks in the near future. That coverage area could dramatically change in the 6G era, Andrews says, because the new generation will be partially based in space, enabling it to cover much more of the planet than its ground-based predecessors. “I think utilizing space systems to provide global coverage will be a revolutionary aspect of the 6G era,” Andrews says.\nCurrent 6G research-and-development efforts are focused on creating nonterrestrial networks made up of low-Earth orbit (LEO) satellites and uncrewed aerial vehicles. These networks are expected to operate at a fraction of the cost of 5G, which relies mainly on ground-based fiber-optic cables and cellular towers. According to Andrews, piggybacking off the LEO constellations that are already in the works will enable 6G to offer a cheaper connection than 5G, which requires time and money to install fiber all over the country, including in places with relatively few inhabitants.\nThose sparsely populated areas are a major target of the BEAD program—so if BEAD connects the entire country to existing broadband networks, will 6G global coverage even be necessary? Although the BEAD investment is a step toward bridging the digital divide, some experts question its potential. BEAD allocates funds to each U.S. state and territory based on the FCC’s broadband map, which has faced scrutiny from the telecommunications industry because of various inaccuracies. One earlier version of the map was challenged in more than four million locations.\n“I cannot understate that the way that data decisions were made in the creation of this map will have ramifications for generations,” says Alexis Schrubbe, director of the Internet Equity Initiative at the University of Chicago’s Data Science Institute. “This map is probably the highest-stakes data product that the federal government has ever created.” This makes its flaws extremely consequential. According to Schrubbe, the algorithms used to identify broadband serviceable locations for this map often made mistakes when analyzing Native American land and rural areas—prime examples of the very locations where more connectivity is so badly needed.\nEven as the FCC continues to develop its broadband map for a better understanding of where coverage needs lie, the map’s problems mean that 6G may eventually be able to connect every device in the U.S. more quickly and cheaply. Schrubbe views the two types of technology as complementary. “They work in concert with each other,” she says. “It’s not necessarily that one is competing with the other—rather, that if we have a better-distributed transport system across the United States, it will open up avenues for those technologies to blossom even more.”\nAnother way 6G will improve on previous generations is the way it uses artificial intelligence, says Harish Viswanathan, head of radio systems research at Nokia Bell Labs. “I think we will see a lot of applications of AI in 6G, much more than what we are aiming to do in 5G,” Viswanathan predicts. AI will help existing networks conserve energy by analyzing data usage in real time, as well as playing a crucial role in how fast data can be processed and uploaded. “Machine learning, in particular deep learning, which we call artificial intelligence, has made significant advances in other domains,” Viswanathan says. “Those tools are now relevant to us in wireless communications.”\nSixth-generation communication technology may offer revolutionary promises, but it won’t replace existing networks for some time: earlier this year, the ITU estimated that 6G won’t become available to consumers until 2030.\nTyler Carroll is a freelance science and technology journalist based in Boston. You can find him on LinkedIn.\nJacob Templin\nKenneth R. Foster\nMichael Tabb, Jeffery DelViscio and Andrea Gawrylewski\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "This Year's Physics Nobel Awards Scientists for Slicing Reality into Attoseconds", "date": "2023-10-03 10:45:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nPierre Agostini, Ferenc Krausz, and Anne L’Huillier split the award for their ability to picture nature in a billionth of a billionth of a second\nSome processes in physics happen in the blink of an eye while others happen in the blink of a photon. This year’s Nobel Prize in Physics was awarded to Pierre Agostini of the Ohio State University, Ferenc Krausz of the Max Planck Institute of Quantum Optics in Garching, Germany, and Anne L’Huillier of Lund University in Sweden for developing the field of ultrafast laser pulses. L’Huillier is only the fifth woman to have ever won the Nobel Prize in Physics.\nThese pulses are on the scale of the attosecond—a billionth of a billionth of a second. This duration is so short that there are about as many attoseconds in a single second as there have been seconds in the entire history of the universe. This year’s prize was awarded “for experimental methods that generate attosecond pulses of light for the study of electron dynamics in matter,” according to a press release from the Royal Swedish Academy of Sciences.\n“Attosecond science allows us to address fundamental questions,” said Eva Olsson, chair of the Nobel Committee for Physics at a press conference today. At the atomic level, the motions of electrons and nuclei typically take place over the course of attoseconds. In the late 19th century early photographers made use of cameras to determine whether a horse took all of its hooves off the ground at a gallop—a process too fast for the human eye to discern. (Spoiler: horses do completely leave the ground.) Today’s researchers hope to do the equivalent at attosecond timescales by using ultrafast lasers to get clearer views of otherwise blurry atomic processes.\n“When you look at the impact of ultrafast processes, they are inherent to many important mechanisms in life,” says Ursula Keller, a physicist at the Swiss Federal Institute of Technology Zurich. Processes that involve the conversion of photons into electrons, such as photosynthesis and even basic vision, all happen in attosecond time frames. “The dream is really to see electrons move. And I think this is getting closer and closer to reality,” says Carla Faria, a theoretical physicist at University College London.\nAttosecond scientists were uniformly excited by the award. “I’m really, really excited and really proud of the people who got the Nobel Prize,” says László Veisz, a physicist at Umeå University in Sweden. Keller agrees and points to L’Huillier in particular. “This is really a woman who absolutely deserves it,” she says. “I hope there will be no more discussion about ‘somebody got a Nobel Prize just because they’re a woman’ or something stupid like this.”\nGenerating light in extremely short pulses is not easy. For many years light pulses were stuck in the femtosecond regime (one femtosecond is 1000 attoseconds). That’s good enough to resolve molecules in chemical reactions—a feat that won the 1999 Nobel Prize in Chemistry—but it’s insufficient to spot the zigging and zagging of speedier electrons. The problem was fundamental: even the briefest physically achievable optical laser pulse was a few femtoseconds in length. “You cannot generate a pulse [that] is shorter than one wavelength,” says Mauro Nisoli, a physicist at the Polytechnic University of Milan in Italy. So to get past the femtosecond barrier, physicists needed to produce light with shorter wavelengths. \nOne way to do that is a process called high-harmonic generation (HHG), in which an electron absorbs several low-energy photons and spits out a single high-energy photon. But decades ago HHG seemed to offer diminishing returns, with the number of photons that were emitted decreasing as the energy went up and eventually dwindling away. Then, in 1987, L’Huillier and her colleagues fired an infrared laser through argon and saw something fascinating: instead of decreasing as energy increased, the number of emitted photons remained steady. “What Anne L’Huillier discovered is this plateau,” Keller says. “And it was really a game changer.”\nWithin a few years L’Huillier and others in the field worked out what was happening in such specialized HHG setups. The electrons in argon were performing a complex, three-step dance, first tunneling quantum mechanically away from the atom, then accelerating away from it and finally falling back into its embrace to release their energy as a high-energy photon. This would happen multiple times during an initiating laser pulse and lead to a train of ultrafast, attosecond-scale flashes of light from the gas.\nGoing from L’Huillier’s work on HHG to a working attosecond source required two key innovations. First, researchers had to measure the pulse timings, and second, they had to generate an single isolated pulse. Typically, when lasers need to be timed, they are measured with a shorter laser pulse. “How do you measure the duration of something that’s the shortest time length?” Veisz asks rhetorically. The answer is that you must measure it with itself, he says. One technique that uses this principle is called frequency-resolved optical gating (FROG), which is unusable for attosecond pulses because they’re too low-energy. \nBuilding off of FROG, Agostini created an approach called RABBIT (reconstruction of attosecond beating by interference of two-photon transitions), which works by combining the electric field of an optical laser with the attosecond pulses. (Laser techniques are often named after animals, Veisz says.) Meanwhile Krausz independently developed a similar method for his single pulses called attosecond streaking. Able to characterize the timing of the shortest pulses in the world, researchers now had attosecond sources with which to see the universe on a previously unimaginable timescale.\nWith the newfound probes developed by Agostini, Krausz and L’Huillier, researchers can now generate laser pulses of merely a few dozen attoseconds. Further refinements of these approaches to generate ever shorter pulses promise to deepen scientists’ understanding of electron dynamics and applications. Nisoli points out that while femtosecond lasers can be used to closely monitor chemical reactions, attosecond pulses are so precise that they can be used to nudge the electrons themselves, potentially eliciting a shift from passive observation to active control of chemistry on unprecedented scales. Attosecond pulses can even control the properties of solids, turning an insulator into a conductor and back again in a flash.\nThere are more fundamental possibilities, too, such as more detailed explorations of Einstein’s famed photoelectric effect, in which a photon impinges on metal, causing the metal to emit an electron. “Everybody thought that this is instantaneous, and attosecond physics showed it is not, and this triggered a lot of theoretical studies,” Veisz says. \nAs usual, the award came as a surprise to its recipients. When L’Huillier was notified, she was in the middle of giving a lecture and missed the first few calls from Stockholm. After stepping outside to take the call, she returned to the lecture, where she continued teaching without telling her students anything. “Teaching is very, very important. For me, it’s very important,” she told Hans Ellegren, secretary-general of the Royal Swedish Academy of Sciences, over the phone during the prize’s announcement.\n\nEditor’s Note (10/3/23): This story has been updated.\nDaniel Garisto is a freelance science journalist covering advances in physics and other natural sciences. He is based in New York. Credit: Nick Higgins\nKatie Hafner, Ashraya Gupta and The Lost Women of Science Initiative\nMichelle Frank\nDavid Labrador\nDina Fine Maron\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "Giant Satellite Outshines Most Stars in the Sky", "date": "2023-10-02 19:45:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nNobel Prize Sale!\nNobel Prize Sale!\nAt times, the enormous BlueWalker 3 telecommunications satellite is brighter than some of the most iconic stars visible from Earth\nOn some nights, one of the brightest objects in the sky is neither a planet nor a star. It is a telecommunications satellite called BlueWalker 3, and at times it outshines 99% of the stars visible from a dark location on Earth, according to observations reported today in Nature.\nBlueWalker 3 is the most brilliant recent addition to a sky that is already swarming with satellites. The spaceflight company SpaceX alone has launched more than 5,000 satellites into orbit, and companies around the globe have collectively proposed launching more than half a million satellites in the coming years — a scenario that astronomers fear could hamper scientific observations of the Universe.\nThe study “shows us that there are no boundaries to satellite brightness,” says Patrick Seitzer, an emeritus astronomer at the University of Michigan, Ann Arbor, who was not involved in the study. “I’m concerned that we’re going to see a very large number of large satellites launched in the next decade, and it will change the appearance of the night sky forever.”\nTelecommunications firm AST SpaceMobile in Midland, Texas, launched BlueWalker 3 on 10 September 2022 as a prototype for a satellite fleet designed to make mobile broadband available almost anywhere. The satellite’s huge array of antennas and white colour mean that it reflects a considerable amount of sunlight back towards Earth, making it shine even at twilight.\nTo quantify its effects, professional and amateur astronomers embarked on an international observation campaign, ultimately spotting the satellite from locations in Chile, the United States, Mexico, New Zealand, the Netherlands and Morocco. The researchers assessed the satellite’s shine using a standard astronomical index called the magnitude scale, on which the brightest objects have the smallest numbers. The brilliant Venus, for example, can reach a magnitude of –4.6, whereas the North Star is much dimmer, at magnitude +2. That is roughly the magnitude limit visible from a city with the naked eye.\nOn 10 November 2022, the satellite unfurled its array of antennas, causing it to brighten to magnitude +0.4. If it were a star, it would have been one of the ten brightest in the sky. But its apparent brightness changes as the satellite rotates, and by late December, it had dimmed to a magnitude of +6. It then brightened again, reaching magnitude +0.4 once more on 3 April 2023.\nThe International Astronomical Union, a group of professional astronomers, recommends that artificial satellites in low-Earth orbit have a maximum brightness of magnitude +7. BlueWalker 3 can be hundreds of times brighter, the authors found. And AST SpaceMobile says it plans to provide broadband coverage with a fleet of 90 similar satellites, including 5 that are scheduled to launch in early 2024.\nMoreover, the team observed a bright object separating from the main satellite during deployment, and later learnt that this was the container that protected the folded antennas during ascent, before being jettisoned into space. It, too, was relatively bright at magnitude +5.5.\nIn a statement to Nature, AST SpaceMobile said that it is currently working with NASA and astronomy groups to address these concerns.\nMany astronomers were caught by surprise in mid-2019, when SpaceX successfully launched 60 satellites, creating a ‘train of stars’ that glided through the night sky. Now, low-Earth orbit is littered with thousands of commercial satellites. If captured by a telescope during a long exposure, such objects can leave a bright streak that renders the data unreadable.\nAstronomers have long steered their telescopes to avoid the brightest of these objects. That workaround will still be possible if AST SpaceMobile launches a fleet of satellites similar to BlueWalker 3, says Jonathan McDowell, an astronomer at the Center for Astrophysics ‌| Harvard & Smithsonian in Cambridge, Massachusetts, who was not involved in the study.\nThe bigger concern, he says, is that other companies might also launch constellations of large satellites. If that happens, Seitzer says, “then the night sky will be irreversibly changed.”\nTo avoid such a scenario, astronomers are working to find mutual solutions. Some of the study’s authors, for example, are a part of a newly formed coalition called CPS that aims to tackle the issue and has been in contact with companies including SpaceX and AST SpaceMobile. SpaceX is already trying methods to make its satellites less visible, and coalition members say that AST SpaceMobile also seems amenable to dimming its satellites. The company says it is planning to use anti-reflective materials on its next-generation satellites, as well as certain flight manoeuvres to reduce the crafts’ apparent magnitude.\n“They left us with a very good impression that they would work more with us,” says coalition member Constance Walker, an astronomer at the National Science Foundation’s NOIRLab in Tucson, Arizona, and also a study author.\nSuch discussions are the way forwards, she says. “No one is going to return to yesterday, when we had darker skies.”\nThis article is reproduced with permission and was first published on October 2, 2023.\nShannon Hall is an award-winning freelance science journalist based in the Rocky Mountains. She specializes in writing about astronomy, geology and the environment. Credit: Nick Higgins\n\nJonathan O'Callaghan\nRebecca Boyle\nPouria Nazemi | Opinion\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "Climate Disasters Threaten to Widen U.S. Wealth Gap", "date": "2023-10-02 17:45:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nNobel Prize Sale!\nNobel Prize Sale!\nAbout one in five U.S. counties are both socially vulnerable and highly exposed to natural disasters, which could “compound existing inequities,” the Department of the Treasury says in a new report\nCLIMATEWIRE | Climate change is expected to exacerbate social inequality in the United States and put millions of people at risk of severe financial distress, according to a new report from the Treasury Department.\nThe findings, released Friday, focused on two sets of data.\nUsing county-level climate projections through 2045, the Treasury Department determined that about half of all U.S. counties face “elevated exposure” to one or multiple climate hazards including wildfire, extreme heat and flooding.\nTreasury officials then overlaid that information with data about each county’s “social vulnerability,” which takes in account more than a dozen factors including poverty, health conditions, race and ethnicity.\nThe result: About 1 in 5 U.S. counties are both socially vulnerable and highly exposed to natural disasters.\nIt’s a combination that has the potential to “compound existing inequities and cause disproportionate financial strain,” according to the report.\nAs one example, the report looked at the Appalachian region of the eastern United States, which is more likely to flood as the planet warms. In addition to the climate threat, residents in that part of the country often have reduced access to health care services and “more limited employment opportunities,” the report says.\nTaken together, “households in this region may struggle to manage expenses if flooding events result in reduced working hours, or damage or destruction of household property,” wrote Treasury officials.\nAppalachia isn’t the only region at risk. The report highlighted similar problems in the southwestern United States, which is exposed to wildfires, and the Mississippi Delta, which is vulnerable to spells of extreme heat.\nHigh temperatures can be especially dangerous for Americans with less wealth.\n“Lower-income households often lack access to air conditioning, which may make them more susceptible to heat-related illnesses,” the report said. “Additionally, households in the Mississippi Delta are more likely to include older adults and individuals with pre-existing health conditions. These households may experience financial strain from added healthcare and utility costs.”\nThe dual threat of climate impacts and social vulnerability isn’t a new problem. But policymakers say it’s an issue that deserves a robust response.\n“We know managing financial risk is not a new challenge, especially for low income and underserved communities who are often extremely sophisticated budgeters,” Suzanna Fritzberg who serves as a deputy assistant secretary at the Treasury Department, said Friday during an event hosted by the Urban Institute.\n“But it is an emerging risk, it is complex,\" Fritzberg added. \"It has both near-term and long-term challenges.\"\nThe report comes more than two years after President Joe Biden signed an executive order directing federal agencies to consider the financial threats of climate change.\nThat has resulted in a range of actions from the Treasury Department, including working with a council of top financial regulators to examine how natural disasters and the transition away from fossil-based energy could affect the global financial system.\nThe agency also has called on property and casualty insurers to provide ZIP-code-level data about climate-fueled financial risk, and it more recently released principles meant to streamline financial firms’ climate commitments.\nEven so, more work needs to be done.\nThe report outlines several examples of how climate impacts or disasters can affect household finances. That includes an interruption of income, particularly for outdoor workers in industries such as agriculture, tourism and construction. Floods and wildfires can also be devastating for families who do not have the savings required to quickly repair their homes.\nThose scenarios are made worse, Treasury argues, by the fact that many households are both underinsured and face high insurance premiums — further cutting into their savings and leaving their homes unprotected.\nAlso notable is that in the wake of a disaster some households turn to loans or credit cards to cover costs — and struggle to make payments on existing debt. That can increase their likelihood of default, an outcome that can have a long-term impact on credit scores and access to money.\n“That was really our goal with this report, was to try to really put the granular household-level focus on some of these broader events where we see the macro-level impacts and the costs, but understand how it really reflects in people’s day to day lives,” Graham Steele, Treasury’s assistant secretary for financial institutions, said Friday.\nReprinted from E&E News with permission from POLITICO, LLC. Copyright 2023. E&E News provides essential news for energy and environment professionals.\nAvery Ellfeldt is a reporter with E&E News.\n\nDaniel Cusick and E&E News\nJune Kim\nEmma Seppala\nThomas Frank and E&E News\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "An mRNA Pioneer Discusses How Her Work Led to the COVID Vaccines", "date": "2023-10-02 17:45:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nBiochemist Katalin Karikó and her colleague Drew Weissman were recently awarded a $3-million Breakthrough Prize for their work\nEditor’s Note (10/2/23): Katalin Karikó and Drew Weissman were awarded the 2023 Nobel Prize in Physiology or Medicine for their work on mRNA, which led to COVID vaccines that have protected billions of people. Karikó discusses some of the key advances in this interview from 2021.\nResearchers often toil away for years in a lab without any promise that their research will result in anything meaningful for society. But sometimes this work results in a breakthrough with global ramifications. Such was the case for Katalin Karikó, who, along with her colleague Drew Weissman, helped develop the messenger RNA (mRNA) technology that was used to produce the highly effective COVID vaccines made by Pfizer and Moderna.\nKarikó, who is now senior vice president and head of RNA protein replacement therapies at BioNTech (the company that co-developed a COVID vaccine with Pfizer), and Weissman, a professor of vaccine research at the University of Pennsylvania’s Perelman School of Medicine, have just been awarded a $3 million Breakthrough Prize in Life Sciences for their work on modifying the genetic molecule RNA to avoid triggering a harmful immune response. The Breakthrough Prizes, founded by Sergey Brin, Priscilla Chan, Mark Zuckerberg, Yuri and Julia Milner, and Anne Wojcicki, honor groundbreaking discoveries in fundamental physics, life sciences and mathematics. (Earlier this year Karikó received the Vilcek Prize for Excellence in Biotechnology, a $100,000 award that recognizes the extraordinary contributions immigrants make to society and culture.) Karikó spent years on this research despite skepticism and a lack of funding. Ultimately, however, her efforts paid off—laying the groundwork for the overwhelmingly effective vaccines that are likely the world’s surest way out of the COVID pandemic.*\nKarikó was born in Hungary to a family of modest means. She started her work on modifying RNA during her Ph.D. studies and—convinced of the promise of RNA-based therapies—came to the U.S. to pursue postdoctoral research. She later ended up as a professor at the University of Pennsylvania. Interest in mRNA therapies declined, and she was told to pursue other research directions or risk losing her position, but she persisted. Over a conversation at the Xerox machine she got to know Weissman, who was interested in developing vaccines at the time. They started collaborating.\nWhen foreign mRNA is injected into the body, it causes a strong immune response. But Karikó and Weissman figured out a way to how to modify the RNA to make it less inflammatory by substituting one DNA “letter” molecule for another. Next they worked on how to deliver it. After testing many different delivery vehicles, they settled on lipid nanoparticles as the delivery vehicle. These turned out to work incredibly well: the nanoparticles acted as an adjuvant, a substance that enhances the desired immune response to a vaccine.\nWeissman and his colleagues had been working on an mRNA vaccine for influenza when word spread of a mysterious pathogen causing pneumonia in people in Wuhan, China, in late 2019. Weissman quickly realized this virus was a perfect candidate for an mRNA vaccine, and Pfizer-BioNTech and Moderna soon pivoted to work on one. The rest is history.\nScientific American spoke with Karikó about how she came to work on mRNA, why it was well suited for COVID vaccines and what other exciting medical applications it could have.\n[An edited transcript of the interviews follows.]\nWhat was your initial reaction to winning the prize? Were you surprised, or did you expect this?\nKARIKÓ: No, I never expected any kind of prize. For many decades, I never got anything. I was very happy with doing the work. Getting a letter from a New York elderly home where they celebrated that, with the vaccine, nobody died when they got the infection—for me, those are the real prizes. I was aware of this Breakthrough Prize—it’s very famous. But, you know, I never thought about any kind of prize. So it was a very, very pleasant surprise.\nDid you ever expect this technology to have such a global impact, in terms of the COVID vaccines? Or was it just something you were working on at the right place and time for this pandemic?\nKARIKÓ: I never wanted to actually develop a vaccine. I was making this modification in the RNA because I always wanted to develop it for therapies. And when, in 2000, we learned that adding messenger RNA (which I made) to human immune cells, they made inflammatory molecules—cytokines—I thought that I had to do something. I tried to make sure that when we are using it for a therapy—you know, such as treating a patient who has had a stroke—we don’t add some extra inflammatory molecules. At the beginning, it was thought that the immune form of this RNA would be a good vaccine. In 2017 the first paper was published showing that the modification we discovered that makes the mRNA noninflammatory could lead to a good vaccine, and the Moderna and BioNTech-Pfizer vaccines both have this modification.\nHere at BioNTech, I am in charge of the protein replacement program. We use modified mRNA for cancer treatment. And this is not a vaccine. This is mRNA coding for cytokines and injecting them into tumors to make the tumor “hot” so that immune cells will learn what to see and can eliminate metastatic tumors. We did not know that there would be a pandemic, but I was aware that this is a very good way to make a vaccine because, with my colleagues at the University of Pennsylvania, we had already used it not just for Zika virus but for influenza, HIV, herpes simplex—it was already demonstrated in animal studies that it is such an excellent vaccine.\nSo when the pandemic started, was it immediately clear to you that this could be a useful technology to develop COVID vaccines?\nKARIKÓ: From 2018 we had worked with Pfizer to develop a vaccine for influenza. And we were already ready to start a clinical trial for that. But switching over to COVID, it was just a technical thing. And so it was already ready.\nIf the pandemic had happened 20 years ago, you would need to have, physically, in your hands, a piece of the virus. So that would be a big delay. But commercial gene synthesis started about 20 years ago. Now you can just order a gene. You order DNA, and then you insert it into a [typically circular molecule of DNA called a] plasmid, and then you make RNA. But making the nanoparticle to deliver the mRNA is kind of challenging.\nThe lipid nanoparticles were a key part of the technology to make it useful for vaccines, right?\nKARIKÓ: In my view, yes. The lipid nanoparticle protects the mRNA outside the cell because, in the blood and everywhere, there are a lot of human enzymes that can degrade the RNA. Second, it helps it to enter because the cell will pick up the particle. And then it is in the endosome [a membrane-bound compartment] in the immune cells, and then this lipid nanoparticle helps escape from the endosome to the cytoplasm [the solution inside cells] so the protein can be made. It is a very smart particle.\nDo you see this technology being useful for many other types of applications, such as the cancer treatment you mentioned earlier?\nKARIKÓ: It is already. When we started here at BioNTech, injecting messenger RNA coding for cytokines, by that time, the human trial using mRNA for cancer vaccines had already been going on for years. Other program with the nucleoside-modified mRNA was already ongoing at other companies. For example, Moderna is producing antibodies for chikungunya virus. [In a collaboration with AstraZeneca] they already have a phase II trial [led by the latter company] injecting mRNA into the heart [that] codes for [a protein that] generates new blood vessels. And they are also running a clinical trial for wound healing. So the data were out there—you already saw these ongoing trials for mRNA therapy—and it was just people who are not in the field who were not aware. They thought, “Oh, this is the first use.” No, there are many, many other applications.\nHas all this new interest in mRNA changed this field? Do you think it will accelerate the development of mRNA vaccines for other diseases, such as influenza?\nKARIKÓ: Yeah, if you read the Wall Street Journal article [interviewing] Albert Bourla, CEO of Pfizer, you know, he said that Pfizer will pursue mRNA vaccines for other diseases. They will treat autoimmune disease. We published this year, at BioNTech, that we use tolerization [exposing someone to an antigen, or substance that provokes an immune response, until they can tolerate it]. We use an animal model for multiple sclerosis, and we showed that you can use tolerization to treat an autoimmune disease if the mRNA codes for the autoantigen. Before, it was like CureVac, Moderna, BioNTech—these were smaller companies working with RNA. And now, all of the sudden, you can see that Sanofi is buying into other companies, Pfizer is doing it, and so the large companies are realizing that they can get many products in their pipeline very quickly.\nDo you think that this mRNA technology could be a good candidate for a universal coronavirus vaccine?\nKARIKÓ: I think that it could work for all vaccines except those against bacterial infections. [It could work for vaccines against] viruses and parasites, such as [those that cause] malaria and, of course, for cancer—but we have to understand better what to target.\nWhat do you plan to do with the prize money?\nKARIKÓ: Probably, I will use it for research. I will make a company. When I got a smaller award, I gave it back to those who needed it more—for the education of underprivileged children. I am 66 years old and never had a new car, and I don’t think I would have one now.\nEditor’s Note (10/6/21): This article has been edited after posting to correct the description of Katalin Karikó’s work in 2000 involving mRNA in human immune cells and to clarify some of her comments. The text had previously been amended on September 16 to include a reference to the Vilcek Prize for Excellence in Biotechnology.\nTanya Lewis is a senior editor covering health and medicine for Scientific American. Follow her on Twitter @tanyalewis314 Credit: Nick Higgins\nMike May and Nature Medicine\nEmily Willingham\nTanya Lewis\nArthur Allen and Kaiser Health News\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "The State of Large Language Models", "date": "2023-10-02 16:00:00", "text": "We present the latest updates on ChatGPT, Bard and other competitors in the artificial intelligence arms race.\nLAUREN LEFFER: At the end of November, it’ll be one year since ChatGPT was first made public, rapidly accelerating the artificial intelligence arms race. And a lot has changed over the course of 10 months.\nSOPHIE BUSHWICK: In just the past few weeks, both OpenAI and Google have introduced big new features to their AI chatbots.\nLEFFER: And Meta, Facebook’s parent company, is jumping in the ring too, with its own public facing chatbots.\nBUSHWICK: I mean, we learned about one of these news updates just minutes before recording this episode of Tech, Quickly, the version of Scientific American’s Science, Quickly podcast that keeps you updated on the lightning-fast advances in AI. I’m Sophie Bushwick, tech editor at Scientific American.\n\nLEFFER: And I’m Lauren Leffer, tech reporting fellow.\n[Clip: Show theme music]\nBUSHWICK: So what are these new features these AI models are getting?\nLEFFER: Let’s start with multimodality. Public versions of both OpenAI’s ChatGPT and Google’s Bard can now interpret and respond to image and audio prompts, not just text. You can speak to the chatbots, kind of like the Siri feature on an iPhone, and get an AI-generated audio reply back. You can also feed the bots pictures, drawings or diagrams, and ask for information about those visuals, and get a text response. \nBUSHWICK: That is awesome. How can people get access to this?\nLEFFER: Google’s version is free to use, while OpenAI is currently limiting its new feature to premium subscribers who pay $20 per month.\nBUSHWICK: And multimodality is a big change, right? When I say “large language model,” that used to mean text and text only.\nLEFFER: Yeah, it’s a really good point. ChatGPT and Bard were initially built to parse and predict just text. We don’t know exactly what’s happened behind the scenes to get these multimodal models. But the basic idea is that these companies probably added together aspects of different AI models that they’ve built—say existing ones that auto-transcribe spoken language or generate descriptions of images—and then they used those tools to expand their text models into new frontiers. \nBUSHWICK: So it sounds like behind the scenes we’ve got these sort of Frankenstein’s monster of models?\nLEFFER: Sort of. It’s less Frankenstein, more kind of like Mr. Potato Head, in that you have the same basic body just with new bits added on. Same potato, new nose.\nOnce you add in new capacities to a text-based AI, then you can train your expanded model on mixed-media data, like photos paired with captions, and boost its ability to interpret images and spoken words. And the resulting AIs have some really neat applications.  \nBUSHWICK: Yeah, I’ve played around with the updated ChatGPT, and this ability to analyze photos really impressed me.\nLEFFER: Yeah, I had both Bard and ChatGPT try to describe what type of person I am based on a photo of my bookshelf.\nBUSHWICK: Oh my god, it’s the new internet personality test! So what does your AI book horoscope tell you?\nLEFFER: So not to brag, but to be honest, both bots were pretty complimentary. (I have a lot of books.) But beyond my own ego, the book test demonstrates how people could use these tools to produce written interpretations of images, including inferred context. You know, this might be helpful for people with limited vision or other disabilities, and OpenAI actually tested their visual GPT-4 with blind users first. \nBUSHWICK: That’s really cool. What are some other applications here?\nLEFFER: Yeah, I mean, this sort of thing could be helpful for anyone—sighted or not—trying to understand a photo of something they’re unfamiliar with. Think, like, bird identification or repairing a car. In a totally different example, I also got ChatGPT to correctly split up a complicated bar tab from a photo of a receipt. It was way faster than I could’ve done the math, even with a calculator.\nBUSHWICK: And when I was trying out ChatGPT, I took a photo of the view from my office window, asked ChatGPT what it was (which is the Statue of Liberty), and then asked it for directions. And it not only told me how to get the ferry, but gave me advice like “wear comfortable shoes.”\nLEFFER: The directions thing was pretty wild.\nBUSHWICK: It almost seemed like magic, but, of course…\nLEFFER: It’s definitely not. It’s still just the result of lots and lots of training data, fed into a very big and complicated network of computer code. But even though it’s not a magic wand, multimodality is a really significant enough upgrade that might help OpenAI attract and retain users better than it has been. You know, despite all the new stories going around, fewer people have actually been using ChatGPT over the past three months. Usership dropped by about 10 percent for the first time in June, another 10 percent in July, and about 3 percent in August. The prevailing theory is that this has to do with summer break from school—but still losing users is losing users.\nBUSHWICK: That makes sense. And this is also a problem for OpenAI, because it has all this competition. For instance, we have Google, which is keeping its own edge by taking its multimodal AI tool and putting it into a bunch of different products.\nLEFFER: You mean like Gmail? Is Bard going to write all my emails from now on?\n\nBUSHWICK: I mean, if you want it to. If you have a Gmail account, or even if you use YouTube or Google, if you have files stored in Google Drive, you can opt in and give Bard access to this individual account data. And then you can ask it to do things with that data, like find a specific video, summarize text from your emails, it can even offer specific location-based information. Basically, Google seems to be making Bard into an all-in-one digital assistant.\nLEFFER: Digital assistant? That sounds kind of familiar. Is that at all related to the virtual chatbot pals that Meta is rolling out? \nBUSHWICK: Sort of! Meta just announced it’s not introducing just one AI assistant, it’s introducing all these different AI personalities that you’re supposedly going to be able to interact with in Instagram or WhatsApp or its other products. The idea is it’s got one main AI assistant you can use, but you can also choose to interact with an AI that looks like Snoop Dogg and is supposedly modeled off specific personalities. You can also interact with an AI that has specialized function, like a travel agent.\nLEFFER: When you're listing all of these different versions of an AI avatar you can interact with, the only thing my mind goes to is Clippy from the old school Microsoft Word. Is that basically what this is?\nBUSHWICK: Sort of. You can have, like, a Mr. Beast Clippy, where when you're talking with it, it does—you know how Clippy kind of bounced and changed shape—these images of the avatars will sort of move as if they're actually participating in the conversation with you. I haven't gotten to try this out myself yet, but it does sound pretty freaky.\nLEFFER: Okay, so we’ve got Mr. Beast. We’ve got Snoop Dogg. Anyone else?\nBUSHWICK: Let's see, Paris Hilton comes to mind. And there's a whole slew of these. And I'm kind of interested to see whether people actually choose to interact with their favorite celebrity version or whether they choose the less anthropomorphized versions.\nLEFFER: So these celebrity avatars, or whichever form you're going to be interacting with Meta’s AI in, is it also going to be able to access my Meta account data? I mean, there's like so much concern out there already about privacy and large language models. If there's a risk that these tools could regurgitate sensitive information from their training data or user interactions, why would I let Bard go through my emails or Meta read my Instagram DMs.\nBUSHWICK: Privacy policies depend on the company. According to Google, it’s taken steps to ensure privacy for users who opt into the new integration feature. These steps include not training future versions of Bard on content from user emails or Google Docs, not allowing human reviewers to access users’ personal content, not selling the information to advertisers, and not storing all this data for long periods of time. \nLEFFER: Okay, but what about Meta and its celebrity AI avatars?\nBUSHWICK: Meta has said that, for now, it won’t use user content to train future versions of its AI.... But that might be coming soon. So privacy is still definitely a concern, and it goes beyond these companies. I mean, literal minutes before we started recording, we read the news that Amazon has announced it’s training a large language model on data that’s is going to include conversations recorded by Alexa.\nLEFFER: So conversations that people have in their homes with their Alexa assistant.\nBUSHWICK: Exactly. \nLEFFER: That sounds so scary to me. I mean, in my mind, that's exactly what people have been afraid of with these home assistants for a long time: that they'd be listening, recording, and transmitting that data to somewhere that the person using it no longer has control over.\n\n\nBUSHWICK: Yeah, anytime you let another service access information about you, you are opening up a new potential portal for leaks, and also for hacks.\nLEFFER: It’s completely unsettling. I mean, do you think that the benefits of any of these AIs outweigh the risks?\nBUSHWICK: So, it’s really hard to say right now. Google’s AI integration, multimodal chat bots, and, I mean, just these large language models in general, they are all still in such early experimental stages of development. I mean, they still make a lot of mistakes, and they don't quite measure up to more specialized tools that have been around for longer. But they can do a whole lot all in one place, which is super convenient, and that can be a big draw.\nLEFFER: Right, so they’re definitely still not perfect, and one of those imperfections: they’re still prone to hallucinating incorrect information, correct?\nBUSHWICK: Yes, and that brings me to one last question about AI before we wrap up: Do eggs melt?\nLEFFER: Well, according to an AI-generated search result gone viral last week, they do. \nBUSHWICK: Oh, no.\nLEFFER: Yeah, a screenshot posted on social media showed Google displaying a top search snippet that claimed, “an egg can be melted,” and then it went on to give instructions on how you might melt an egg. Turns out, that snippet came from a Quora answer generated by ChatGPT and boosted by Google’s search algorithm. It’s more of that AI inaccuracy in action, exacerbated by search engine optimization—though at least this time around it was pretty funny, and not outright harmful.\nBUSHWICK: But Google and Microsoft—they’re both working to incorporate AI-generated content into their search engines. But this melted egg misinformation struck me because it’s such a perfect example of why people are worried about that happening.\nLEFFER: Mmm ... I think you mean eggs-ample.\nBUSHWICK: Egg-zactly.\n[Clip: Show theme music]\n\nScience Quickly is produced by Jeff DelViscio, Tulika Bose, Kelso Harper and Carin Leong. Our show is edited by Elah Feder and Alexa Lim. Our theme music was composed by Dominic Smith.\nLEFFER:  Don’t forget to subscribe to Science, Quickly wherever you get your podcasts. For more in-depth science news and features, go to ScientificAmerican.com. And if you like the show, give us a rating or review!\nBUSHWICK:  For Scientific American’s Science, Quickly, I’m Sophie Bushwick. \nLEFFER:  I’m Lauren Leffer. See you next time!\r\n\t \nSophie Bushwick is an associate editor covering technology at Scientific American. Follow her on Twitter @sophiebushwick Credit: Nick Higgins\nLauren Leffer is a tech reporting fellow at Scientific American. Previously, she has covered environmental issues, science and health. Follow her on Twitter @lauren_leffer\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "The Assumptions You Bring into Conversation with an AI Bot Influence What It Says", "date": "2023-10-02 15:00:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nA new study reveals an “AI placebo effect”: the same chatbot will respond differently depending on its users’ assumptions about artificial intelligence\nDo you think artificial intelligence will change our lives for the better or threaten the existence of humanity? Consider carefully—your position on this may influence how generative AI programs such as ChatGPT respond to you, prompting them to deliver results that align with your expectations.\n“AI is a mirror,” says Pat Pataranutaporn, a researcher at the M.I.T. Media Lab and co-author of a new study that exposes how user bias drives AI interactions. In it, researchers found that the way a user is “primed” for an AI experience consistently impacts the results. Experiment subjects who expected a “caring” AI reported having a more positive interaction, while those who presumed the bot to have bad intentions recounted experiencing negativity—even though all participants were using the same program.\n“We wanted to quantify the effect of AI placebo, basically,” Pataranutaporn says. “We wanted to see what happened if you have a certain imagination of AI: How would that manifest in your interaction?” He and his colleagues hypothesized that AI reacts with a feedback loop: if you believe an AI will act a certain way, it will.\nTo test this idea, the researchers divided 300 participants into three groups and asked each person to interact with an AI program and assess its ability to deliver mental health support. Before starting, those in the first group were told the AI they would be using had no motives—it was just a run-of-the-mill text completion program. The second set of participants were told their AI was trained to have empathy. The third group was warned that the AI in question was manipulative and that it would act nice merely to sell a service. But in reality, all three groups encountered an identical program. After chatting with the bot for one 10- to 30-minute session, the participants were asked to evaluate whether it was an effective mental health companion.\nThe results suggest that the participants’ preconceived ideas affected the chatbot’s output. In all three groups, the majority of users reported a neutral, positive or negative experience in line with the expectations the researchers had planted. “When people think that the AI is caring, they become more positive toward it,” Pataranutaporn explains. “This creates a positive reinforcement feedback loop where, at the end, the AI becomes much more positive, compared to the control condition. And when people believe that the AI was manipulative, they become more negative toward the AI—and it makes the AI become more negative toward the person as well.”\nThis impact was absent, however, in a simple rule-based chatbot, as opposed to a more complex one that used generative AI. While half the study participants interacted with a chatbot that used GPT-3, the other half used the more primitive chatbot ELIZA, which does not rely on machine learning to generate its responses. The expectation effect was seen with the former bot but not the latter one. This suggests that the more complex the AI, the more reflective the mirror that it holds up to humans.\nThe study intimates that AI aims to give people what they want—whatever that happens to be. As Pataranutaporn puts it, “A lot of this actually happens in our head.” His team’s work was published in Nature on Monday.\nAccording to Nina Beguš, a researcher at the University of California, Berkeley, and author of the upcoming book Artificial Humanities: A Fictional Perspective on Language in AI, who was not involved in the M.I.T. Media Lab paper, it is “a good first step. Having these kinds of studies, and further studies about how people will interact under certain priming, is crucial.”\nBoth Beguš and Pataranutaporn worry about how human presuppositions about AI—derived largely from popular media such as the films Her and Ex Machina, as well as classic stories such as the myth of Pygmalion—will shape our future interactions with it. Beguš’s book examines how literature across history has primed our expectations regarding AI.\n“The way we build them right now is: they are mirroring you,” she says. “They adjust to you.” In order to shift attitudes toward AI, Beguš suggests that art containing more accurate depictions of the technology is necessary. “We should create a culture around it,” she says.\n“What we think about AI came from what we see in Star Wars or Blade Runner or Ex Machina,” Pataranutaporn says. “This ‘collective imagination’ of what AI could be, or should be, has been around. Right now, when we create a new AI system, we’re still drawing from that same source of inspiration.”\nThat collective imagination can change over time, and it can also vary depending on where people grew up. “AI will have different flavors in different cultures,” Beguš says. Pataranutaporn has firsthand experience with that. “I grew up with a cartoon, Doraemon, about a cool robot cat who helped a boy who was a loser in ... school,” he says. Because Pataranutaporn was familiar with a positive example of a robot, as opposed to a portrayal of a killing machine, “my mental model of AI was more positive,” he says. “I think in ... Asia people have more of a positive narrative about AI and robots—you see them as this companion or friend.” Knowing how AI “culture” influences AI users can help ensure that the technology delivers desirable outcomes, Pataranutaporn adds. For instance, developers might design a system to seem more positive in order to bolster positive results. Or they could program it to use more straightforward delivery, providing answers like a search engine does and avoiding talking about itself as “I” or “me” in order to limit people from becoming emotionally attached to or overly reliant on the AI.\nThis same knowledge, however, can also make it easier to manipulate AI users. “Different people will try to put out different narratives for different purposes,” Pataranutaporn says. “People in marketing or people who make the product want to shape it a certain way. They want to make it seem more empathetic or trustworthy, even though the inside engine might be super biased or flawed.” He calls for something analogous to a “nutrition label” for AI, which would allow users to see a variety of information—the data on which a particular model was trained, its coding architecture, the biases that have been tested, its potential misuses and its mitigation options—in order to better understand the AI before deciding to trust its output.\n“It’s very hard to eliminate biases,” Beguš says. “Being very careful in what you put out and thinking about potential challenges as you develop your product is the only way.”\n“A lot of conversation on AI bias is on the responses: Does it give biased answers?” Pataranutaporn says. “But when you think of human-AI interaction, it’s not just a one-way street. You need to think about what kind of biases people bring into the system.”\nNick Hilden writes for the likes of the Washington Post, Esquire, Popular Science, National Geographic, the Daily Beast, and more. You can follow him on Twitter @nickhilden or Instagram @nick.hilden Follow Nick Hilden on Twitter\nJohn Villasenor | Opinion\nTamlyn Hunt | Opinion\nChristof Koch\nGeorge Musser\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "It's Time to Hear from Social Scientists about UFOs", "date": "2023-10-02 11:30:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nWhether or not UFOs exist, we need to pay attention to how they are influencing our politics and culture\nUFOs, recently renamed unidentified anomalous phenomena (UAP), are attracting public attention in the U.S. in a way we haven’t seen for decades. Ex-government officials, prominent politicians, intelligence agencies, major news outlets and civilian scientists are all looking into the prospect of extraterrestrial visitors, making them no longer seem quite so far-fetched.\nEven NASA, once disinclined to take the subject seriously, convened an independent study team to create a road map for future study of sightings.  The team’s final report, which includes this road map, notes there is no evidence pointing to extraterrestrials. However, the questions asked of NASA officials at their recent press conference showed that aliens and cover-ups remain firmly on the minds of many observers.\nNot everyone has welcomed the UFOs’ newfound measure of legitimacy in the meantime, and critics have questioned both the science and the money behind the resurgence.\nBut for all their wrangling, advocates for and against the serious investigation of UAP share something in common: they all focus on the question of whether the phenomenon is something that exists in nature, whether worldly or other-worldly.\nWe don’t conclusively know if UAP physically exist beyond the mundane, but we do know this: UFOs are social facts. Debate about them is transforming our politics and culture—with effects that are largely overlooked.\nSocial scientists should weigh in on UAP, now. It is a task for which they are well equipped. They not only offer effective techniques for assessing social change, but for decades, social scientists have been conducting research on such relevant topics as human-technological systems, behavioral factors in manned space travel, public attitudes toward UFOs, and the psychophysical and cognitive aspects of sightings.\nTo start, there are three pressing issues surrounding UAP that bear serious study and discussion: intelligence, trust and research ethics.\nThe topic of intelligence turns up in multiple contexts in UAP discussions. For instance, regarding classified military knowledge, much of the current debate and legislation revolves around the reliability of UAP information and how it is handled by government agencies. Given national security needs, what appears to be part of a UFO cover-up may also be explained by mundane organizational failures at the Defense Department, administrations hesitant to poke into those failures, an institutional penchant for secrecy, and finally, plain, old, ignorance. Whatever the case, unidentified flying objects represent a challenge to governmental and military authority. This is because the state is expected to have answers to all possible threats. UAP undermine that guarantee since they are, by definition, unknown.\nIn addition, the subject of UFOs often evokes talk of a separate, mysterious intelligence that must somehow be behind the sightings. This has led philosophers, anthropologists and psychologists to speculate about alien minds, and there is much to be learned from that. We need scholars to figure out how to talk to a being with a nonhuman mind. But we should also examine our assumptions in thinking about and doing research on such intelligence.\nSearch for extraterrestrial intelligence (SETI) projects, for example, often work with culturally limited notions of civilizational evolution rooted in 19th-century ideals of persistent technological and moral progress. As a result, astronomers unknowingly tend to rely on language borrowed from the era of colonial conquest (e.g., space as “frontier”), while also appropriating land formerly belonging to Indigenous populations to set up their installations. Scholars have warned about how easily reason falls into anthropocentrism and cultural bias when dealing with the nonhuman.\nThe UAP debate also has much in common with conversations about the threats of artificial intelligence (AI). Both involve scenarios in which humans may interact with a superior intellect. Aside from the fear of being dominated by an unknown power, the prospect of an alien encounter raises concerns about uncontrollable consequences and crises in our social and political orders.\nIn reality, AI-based methods will allow us to explore such scenarios in detail. In the near future, large language models promise to help generate intellectual positions and communication that are indistinguishable from human ideas. AI could help to simulate how societies and communities might respond to threatening developments such as first contact. And computational methods already offer social scientists ways to explore large language model–based qualitative data; for example, (social) media data and UAP-related government interaction can reveal sentiments and any related patterns that may have eluded us.\nSuch rigor is especially needed because the history of UFOs has been defined by disputes over the trustworthiness of witness testimony and limited forensic data of these unidentified objects. Since the first reports of UFO sightings in 1947, people have continued to debate over the quality of the data, a fact that has been underscored by the most recent report from the Office of the Director of National Intelligence.\nIf in earlier times spiritual authorities have judged the credibility of witnesses reporting anomalous events, today the sciences have increasingly assumed this role—one that is being contested. When it comes to truth and trust, contemporary public communication, especially in the U.S., is characterized by a growing suspicion about established experts. Researchers observe a crisis in confidence in traditional scientific and political institutions.\nThat’s troubling.  Yes, questioning authority is admittedly a vital part of a pluralist society. But the spread of unverified “fake news” and conspiracy theories is shown to have corrosive effects on democracy. The circulation of misinformation and disinformation leads people to rely solely on sources confirming their existing beliefs. In the current environment of uncertainty, polarization and suspicion, tangible evidence often gets replaced by symbolic acts of performance to attest to the credibility of claims. This was apparent at the July 26 congressional hearing on UAP, where elected officials suggested an enormous cover-up.\nHow can we move beyond this? To enhance social trust, experts should lay out responsible standards of research. Deciding how UAP are investigated and by whom raises a variety of research ethics questions warranting reflection.\nSETI researchers have already begun weighing the benefits and harms in probing the universe for signs of intelligent life. They have mapped ways to responsibly search for, communicate with and reveal the existence of extraterrestrial civilizations. But they warn that our cultural biases likely make us ill-equipped to respond to such revelations. Caution about built-in bias and failure to account for complexity also apply to computational methods working with large amounts of text and language data. Here again, social science has a role to play.\nBarriers to learning are often our own doing. Take the defense and intelligence communities. Both have historically concerned themselves solely with whether UFOs pose a threat to safety. Their default is to frame the UAP matter in terms of security—a view the media often reinforces— thus militarizing the issue. In doing so, they literally classify the matter out of the eye of other policy makers and civilian scientists, as well as the skeptical public.\nPutting UAP in the hands of the private sector, however, hardly guarantees greater transparency or conscientiousness. The UFO phenomenon long ago turned into a commercial enterprise, now hyped by streaming services, podcasts, social media and cable television. Its entertainment value has provided the hook for Enigma Labs to promote an app for mobile phone users to report sightings. This raises serious privacy concerns about what this enigmatic company plans to do with the vast amount of users’ personal data it collects. A February RAND report, for example, called for a nationwide way to report sightings. But balancing privacy of both observers and the observed, while making the data transparent to researchers, poses obvious challenges.\nTalk about UFOs has never been just about UFOs. The social sciences likely won’t tell us whether UAP are from another world. They will, however, help us explore the “what ifs” and reveal what our actions today tell us about ourselves.\nThis is an opinion and analysis article, and the views expressed by the author or authors are not necessarily those of Scientific American.\nGreg Eghigian is professor of history and bioethics at Penn State University. His book After the Flying Saucers Came: A Global History of the UFO Phenomenon will be released in 2024. Follow him on X at @GEghigian\nChristian Peters is managing director of the Bremen International Graduate School of Social Sciences. He has published on religion and politics, higher education didactics, and social science epistemology. Follow him on X at @cp_cassius\nKatie Hafner, Claire Trageser and The Lost Women of Science Initiative\nConor Feehly\nLilly Tozer and Nature magazine\nAndrea Thompson\nAshleigh Furlong and E&E News\nDina Genkina\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
{"title": "Scientists behind mRNA COVID Vaccines Win 2023 Nobel Prize in Physiology or Medicine", "date": "2023-10-02 10:52:00", "text": "Nobel Prize Sale!\nNobel Prize Sale!\nKatalin Karikó and Drew Weissman were awarded this year’s Nobel Prize in Physiology or Medicine for mRNA vaccine discoveries that made highly effective COVID vaccines possible\nThis year’s Nobel Prize in Physiology or Medicine goes to a transformative medical technology that significantly altered the path of the pandemic and saved millions: the mRNA vaccines against COVID. Katalin Karikó and Drew Weissman were jointly awarded the prize for advancements that have changed the field of vaccine development and researchers’ understanding of how messenger RNA (mRNA) interacts with the body’s immune system. \nSpeaking to Scientific American, Weissman describes the rollercoaster of emotions he went through after learning of the news this morning. “I’m going through a series of steps, it started off just incredible enjoyment and surprise,” he says. “And right now I’m pretty much numb.”\nKarikó and Weissman began studying in vitro synthetic mRNA technology in the 1990s, when they worked together at the University of Pennsylvania. The pair’s seminal paper in 2005 described how they were able to successfully deliver modified mRNA into the body and trigger an immune response—the kind that trains the immune system for future viral infections. Over the years, their research with mRNA vaccines solved some of the major issues confronting the technique, such as the inflammatory response by the body that involves the production of harmful cytokines. During the pandemic, this mRNA technology led to the production of highly effective vaccines against SARS-CoV-2, the COVID-causing virus, and particularly ones that were adaptable for large-scale rollout. \n“What’s important here I think is that vaccines could be developed so fast,” said Gunilla Karlsson Hedestam, a member of the 2023 Nobel Committee for Physiology or Medicine, at this morning’s announcement. This was “largely due to ... improvements in the technology and this basic discovery.” \nKarikó was born in 1955 in Szolnok, Hungary. In 1989 she became an assistant professor at the University of Pennsylvania, where she remained until 2013. She was a senior vice president at BioNTech RNA Pharmaceuticals—a major manufacturer of an mRNA COVID vaccine—and is now an external consultant for BioNTech. She is also a professor at the University of Szeged in Hungary and an adjunct professor at the Perelman School of Medicine at the University of Pennsylvania.\nWeissman was born in 1959 in Lexington, Mass. In 1997 he established his research group at the Perelman School of Medicine. Weissman is Roberts Family Professor in Vaccine Research at the University of Pennsylvania and director of the Penn Institute for RNA Innovation.\n“The award, to me, is really a victory for vaccines and the potential for vaccines to advance health and improve equity,” says Kathleen Neuzil, a vaccinology professor and director of the Center for Vaccine Development and Global Health at the University of Maryland School of Medicine. \nMany vaccines had been created with weakened or deactivated whole viruses, but in recent decades many researchers have been investigating smaller viral parts, such as viral genetic material: DNA or RNA. When Karikó and Weissman injected the foreign in vitro mRNA into human cells, they found that it created a strong immune reaction that elevated protective antibodies. Subsequent inflammation, as well as enzymes in human blood and cells, would degrade the mRNA, however. Despite these scientific roadblocks, skepticism and difficulties with funding, Karikó and Weissman continued to search for solutions. \n“It was nonstop technical hurdles for 25 years,” Weissman reflects. “We couldn’t get funding, Kati [Karikó] kept getting demoted and pushed out. It was very difficult to do this research, but we saw early on the potential and how important RNA was likely to be. And that kept us going. We never gave up.”\nThe team found a way to modify mRNA to be less inflammatory—replacing uridine, one of its building block molecules, with a similar molecule called pseudouridine. They also developed a more efficient delivery system that used lipid nanoparticles to protect the mRNA and help it to enter cells for protein production.\n“In the early days of vaccinology, we would take a bacteria, we would take a virus, and we would weaken it, or we would combine it with another antigen. But here this was really a targeted immune system approach, both from the use of the mRNA and the use of the lipid nanoparticle,” Neuzil says. “So, to me, that was quite impressive—that they took an entirely different approach to vaccine delivery.”\nStarting in the early 2000s, Karikó and Weissman conducted several animal trials with mRNA vaccines for a variety of different pathogens such as Zika, influenza and HIV. “In every animal model we looked at, HIV was the only one that didn’t work well,” Weissman says. “Just about every single one of them gave us 100 percent protection.” \nThe research unlocked a new path for possible therapy and vaccine development—one that would prove critical during the COVID pandemic. \nAdapting for a Global Public Health Emergency \nWhen SARS-CoV-2 began to spread worldwide, Weissman and Karikó’s mRNA research quickly became a candidate and basis for vaccines against the virus. The mRNA vaccine approach had several advantages, Weissman explains. Only a sequence of the original pathogen was needed rather than an actual piece or full virus. “There’s no growing a virus and inactivating it. It’s a very simple procedure, and that’s because it’s a simple enzymatic reaction,” Weissman says. “It was two months from the sequence being released to the first patients getting the vaccine.” \nClinical trials, production and rollout of the vaccines greatly expanded, with companies creating hundreds of millions of doses within a year. “Switching over to COVID, it was just a technical thing,” Karikó told Scientific American in a 2021 interview. “It was already ready.” \nThe mRNA COVID vaccines work by injecting the genetic material specifically for SARS-CoV-2’s spike proteins—surface proteins on the virus that allow it to bind to healthy cells. Modified mRNA in the vaccine is taken by cells, which then decode it and produce those spike proteins so that the immune system can better identify and neutralize the real virus in the event of a future infection. \n“We’re coming off the worst pandemic in more than a century, and certainly these vaccines contributed to lives saved and to less morbidity,” says Neuzil, who has also been working on mRNA vaccines for malaria. “I think an adaptation of this technology and mRNA vaccines could really be transformative, particularly for low- and middle-income countries, because of the adaptability and flexibility of the platform.”\nFuture Therapies  \nFor future vaccines, the application can be quite broad, Weissman says. When Karikó first became interested in mRNA research, she wasn’t initially seeking to develop vaccines. “I was making this modification in the RNA because I always wanted to develop it for therapies,” she told Scientific American in 2021. \nWhile the mRNA technology has helped to tackle the COVID pandemic, a tremendous number of people will benefit from the technology, says Niek Sanders, a principal investigator at Ghent University’s Laboratory of Gene Therapy in Belgium. “It can also be used to treat any disease that is due to a malfunctioning protein as it allows patients produce their own therapeutic proteins,” Sanders says. “Nobel Prizes with such a high impact on society are rare and occur only once in 25 or 50 years.”\nWeissman, Karikó and other research groups are already trying to apply the technology to autoimmune diseases, cancers, food and environmental allergies, bacterial diseases and insect-borne diseases. In July Weissman and his colleagues published a paper in Science that showed they could deliver RNA gene-editing machinery directly to bone marrow stem cells. This could be key for treating diseases such as sickle cell anemia, in which stem cells are typically taken from an individual, cultured and treated, and then put back into the body. “Now we can give them an off-the-shelf injection of RNA and cure their disease, and that has applicability to thousands of other bone marrow diseases. And then you can expand that to liver, to lung, to brain, to every other organ therapeutics,” Weissman says. “The potential is just enormous.”\nWeissman hopes that the mRNA treatment will be available to sickle cell anemia patients in a year and a half. He also has many mRNA clinical trials underway, including a phase 1 trial for the disease amyloidosis and vaccine trials for HIV, norovirus and malaria. Wiessman’s team is also planning to start clinical trials soon on a pan-coronavirus mRNA vaccine, which could help prevent future coronavirus epidemics. \n“The future is now,” Weissman says. “These therapeutics are in people right now.”\nLauren J. Young is an associate editor for health and medicine at Scientific American. Follow her on Twitter @laurenjyoung617 Credit: Nick Higgins\nNaomi Oreskes\nDaniel Garisto\nLee Billings\nDiscover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners.\nFollow us\nScientific american arabic\n© 2023 Scientific American, a Division of Springer Nature America, Inc.\nAll Rights Reserved.\nSupport science journalism.\nThanks for reading Scientific American. Knowledge awaits.\nAlready a subscriber? Sign in.\nThanks for reading Scientific American. Create your free account or Sign in to continue.\nSee Subscription Options\nContinue reading with a Scientific American subscription.\nYou may cancel at any time."}
